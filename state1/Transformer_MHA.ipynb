{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369843da",
   "metadata": {},
   "source": [
    "# Attention is all your need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cda207",
   "metadata": {},
   "source": [
    "## 概念补充\n",
    "### 位置编码\n",
    "- 位置编码（PE）：由于 Transformer 不像 RNN 那样具有时间序列性质，需通过位置编码（Positional Encoding）保留输入序列的顺序信息。\n",
    "    - 使用正弦和余弦函数为每一个位置生成唯一编码：\n",
    "        - 公式为：$$\\begin{aligned}\n",
    "    P E_{(p o s, 2 i)} & =\\sin \\left(\\frac{p o s}{10000^{2 i / d_{\\text {model }}}}\\right), \\\\\n",
    "    P E_{(p o s, 2 i+1)} & =\\cos \\left(\\frac{p o s}{10000^{2 i / d_{\\text {model }}}}\\right) .\n",
    "    \\end{aligned}$$其中pos是位置索引，i是维度索引，$d_{model}$是嵌入向量的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ea832",
   "metadata": {},
   "source": [
    "## 正则化方法\n",
    "- **Dropout**：对每个子层的输出进行随机丢弃（例如，Add & Nrom）\n",
    "- **平滑标签(Label Smoothing)**：\n",
    "    - 硬标签（hard labels）我们理解的标签属于这个范畴。本质上是一种 one-hot（[0,0,1,0,...,0]）\n",
    "    - 软标签（soft labels）由于硬标签对于模型训练比较极端（softmax只有 logits $\\rightarrow \\infty$ 才逼近1）\n",
    "    - **概念**：正确类别稍小于1（$1 - \\epsilon_{ls}$），其他稍大于0 （所有类别平分 $\\epsilon_{ls}$ ）。\n",
    "        - 具体来说 $$ y = [0, 0, ..., 1, ..., 0] \\rightarrow y' = [\\frac{\\epsilon_{ls}}{C}, \\frac{\\epsilon_{ls}}{C}, ... , 1 - \\epsilon_{ls} + \\frac{\\epsilon_{ls}}{C}, ..., \\frac{\\epsilon_{ls}}{C} ] $$\n",
    "\n",
    "```python \n",
    "smooth = (1 - epsilon) * one_hot + epsilon / C\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a25a1a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## BLEU 分数\n",
    "- BLEU（Bilingual Evaluation Understudy） 双语评估替换\n",
    "- 公式：\n",
    "    $$\n",
    "    BLEU = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)\n",
    "    $$\n",
    "    其中 $BP$ 是 brevity penalty（简短惩罚），$N$ 是 n-gram 的最大长度（通常为 4），$w_n$ 是权重（通常取均匀权重 $w_n = \\frac{1}{N}$），$p_n$ 是 n-gram 精度\n",
    "\n",
    "\n",
    "- 首先要明确两个概念：\n",
    "    - N-gram：用来描述句子中的一组 n 个连续的单词。比如，\"Thank you so much\" 中的 n-grams：\n",
    "        - 1-gram: \"Thank\", \"you\", \"so\", \"much\"\n",
    "        - 2-gram: \"Thank you\", \"you so\", \"so much\"\n",
    "        - 3-gram: \"Thank you so\", \"you so much\"\n",
    "        - 4-gram: \"Thank you so much\" 需要注意的一点是，n-gram 中的单词是按顺序排列的，所以 \"so much Thank you\" 不是一个有效的 4-gram。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ff23b",
   "metadata": {},
   "source": [
    "##  Brevity Penalty\n",
    "\n",
    "这里引出了 **brevity penalty**，这是一个惩罚因子，公式如下：\n",
    "\n",
    "$$BP = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } c > r \\\\ \n",
    "e^{1 - \\frac{r}{c}} & \\text{if } c \\leq r \n",
    "\\end{cases}$$\n",
    "\n",
    "其中 $c$ 是 candidate 的长度，$r$ 是 reference 的长度。\n",
    "\n",
    "当候选译文的长度 $c$ 等于参考译文的长度 $r$ 的时候，$BP = 1$，当候选翻译的文本长度较短的时候，用 $e^{1 - \\frac{r}{c}}$ 作为 BP 值。\n",
    "\n",
    "回到原来的公式：$$BLEU = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)$$，汇总一下符号定义：\n",
    "\n",
    "- $BP$ 文本长度的惩罚因子\n",
    "- $N$ n-gram 中 $n$ 的最大值\n",
    "- $w_n$ 权重\n",
    "- $p_n$ n-gram 的精度 (precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c45f1de",
   "metadata": {},
   "source": [
    "# Attention scores\n",
    "给定查询矩阵 $Q$、键矩阵 $K$ 和值矩阵 $V$，其注意力输出的数学表达式如下：\n",
    "\n",
    "$$Attention(Q, K, V) = Softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "- $Q$ (Query)：用于查询的向量矩阵。\n",
    "- $K$ (Key)：表示键的向量矩阵，用于与查询匹配。\n",
    "- $V$ (Value)：值矩阵，注意力权重最终会作用在该矩阵上。\n",
    "- $d_k$：键或查询向量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef933339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cu118'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e50f7d9",
   "metadata": {},
   "source": [
    "缩放点积注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c118675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    缩放点积注意力计算。\n",
    "    \n",
    "    参数:\n",
    "        Q: 查询矩阵 (batch_size, seq_len_q, embed_size)\n",
    "        K: 键矩阵 (batch_size, seq_len_k, embed_size)\n",
    "        V: 值矩阵 (batch_size, seq_len_v, embed_size)\n",
    "        mask: 掩码矩阵，用于屏蔽不应该关注的位置 (可选)\n",
    "\n",
    "    返回:\n",
    "        output: 注意力加权后的输出矩阵\n",
    "        attention_weights: 注意力权重矩阵\n",
    "    \"\"\"\n",
    "    # get d_k\n",
    "    embed_size = Q.size(-1)\n",
    "\n",
    "    # compute product / importance! divide sqrt(d_k)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(embed_size)\n",
    "\n",
    "    # 如果是带掩码的attention计算，掩码设置为负无穷\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "\n",
    "    # 在最后一维计算注意力分数, key value to probs\n",
    "    # seq_len_k == seq_len_v \n",
    "    attention_weight = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weight, V) \n",
    "     \n",
    "    return output, attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c396c3",
   "metadata": {},
   "source": [
    "- mask\n",
    "    - padding：忽略填充（对齐序列长度）的位置\n",
    "    - look-ahead mask：防止偷看正确答案(TransformerBlock 里面的 Masked Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90097a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.]]]) \n",
      "K tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.],\n",
      "         [12., 13., 14., 15.]]]) \n",
      "V tensor([[[ 0.,  1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.,  7.],\n",
      "         [ 8.,  9., 10., 11.],\n",
      "         [12., 13., 14., 15.]]])\n",
      "output: tensor([[[12.0000, 13.0000, 14.0000, 15.0000],\n",
      "         [12.0000, 13.0000, 14.0000, 15.0000],\n",
      "         [12.0000, 13.0000, 14.0000, 15.0000]]]) \n",
      "attention_weight: tensor([[[2.3195e-16, 3.7751e-11, 6.1442e-06, 9.9999e-01],\n",
      "         [0.0000e+00, 6.0546e-39, 7.7811e-20, 1.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 9.8542e-34, 1.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "Q = torch.arange(12, dtype=torch.float32).view(1, 3, 4)\n",
    "K = torch.arange(16, dtype=torch.float32).view(1, 4, 4 )\n",
    "V = K\n",
    "print(\"Q:\",Q,\"\\nK\", K,\"\\nV\", V)\n",
    "output, attention_weight = scaled_dot_product_attention(Q,K,V)\n",
    "print(\"output:\", output, \"\\nattention_weight:\",attention_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650aab9a",
   "metadata": {},
   "source": [
    "## 单头注意力以及自注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86c056c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        单头注意力机制。\n",
    "        \n",
    "        参数:\n",
    "            embed_size: 输入序列（Inputs）的嵌入（Input Embedding）维度，也是论文中所提到的d_model。\n",
    "        \"\"\"\n",
    "        super(SingleHeadAttention, self).__init__()\n",
    "        self.W_q = nn.Linear(embed_size, embed_size)\n",
    "        self.W_k = nn.Linear(embed_size, embed_size)\n",
    "        self.W_v = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播，\n",
    "        输入：\n",
    "            qkv矩阵，以及mask\n",
    "        返回：\n",
    "            注意力分加权的输出，注意力权重\n",
    "        \"\"\"\n",
    "        Q = self.W_q(q)\n",
    "        K = self.W_k(k)\n",
    "        V = self.W_v(v)\n",
    "        output, attention_weight = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        return output, attention_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e9244",
   "metadata": {},
   "source": [
    "继续忽视多头（multi-head）观察输入，线条一分为三传入 Attention 模块，这意味着查询（query）、键（key）和值（value）实际上都来自同一输入序列**x**\n",
    "![selfattention](./img/Self-attention.png)\n",
    "```python\n",
    "# 定义线性层，用于生成查询、键和值矩阵\n",
    "self.w_q = nn.Linear(embed_size, embed_size)\n",
    "self.w_k = nn.Linear(embed_size, embed_size)\n",
    "self.w_v = nn.Linear(embed_size, embed_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b15fe94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        自注意力（Self-Attention）机制。\n",
    "        \n",
    "        参数:\n",
    "            embed_size: 输入序列的嵌入维度（每个向量的特征维度）。\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.attention = SingleHeadAttention(embed_size)  # 使用通用Attention模块\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            x: 输入序列 (batch_size, seq_len, embed_size)\n",
    "            mask: 掩码矩阵 (batch_size, seq_len, seq_len)\n",
    "\n",
    "        返回:\n",
    "            out: 自注意力加权后的输出 (batch_size, seq_len, embed_size)\n",
    "            attention_weights: 注意力权重矩阵 (batch_size, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # 在自注意力机制中，q, k, v 都来自同一输入序列\n",
    "        # q = k = v = x\n",
    "        out, attention_weights = self.attention(x, x, x, mask)\n",
    "\n",
    "        return out, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33594f0e",
   "metadata": {},
   "source": [
    "## 交叉注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e445d1",
   "metadata": {},
   "source": [
    "![](./img/CrossAttention.png)\n",
    "- block的顺序是 self_attention $\\rightarrow$ cross_attention\n",
    "- 也即从encoder获取kv的信息\n",
    "    - 数学表达式：\n",
    "    $$Q = X_{\\text{decoder}} W^Q, \\quad K = X_{\\text{encoder}} W^K, \\quad V = X_{\\text{encoder}} W^V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e4404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\"\n",
    "        交叉注意力\n",
    "        参数：\n",
    "            embed_size：输入序列的嵌入维度\n",
    "        \"\"\"\n",
    "        super(CrossAttention,self).__init__()\n",
    "        self.attention = SingleHeadAttention(embed_size)\n",
    "    def forward(self, q, kv, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            query: 查询矩阵的输入 (batch_size, seq_len_q, embed_size)\n",
    "            kv: 键和值矩阵的输入 (batch_size, seq_len_kv, embed_size)\n",
    "            mask: 掩码矩阵 (batch_size, seq_len_q, seq_len_kv)\n",
    "\n",
    "        返回:\n",
    "            out: 注意力加权后的输出 (batch_size, seq_len_q, embed_size)\n",
    "            attention_weights: 注意力权重矩阵 (batch_size, seq_len_kv, seq_len_kv)\n",
    "        \"\"\"\n",
    "        # 根据数学表达式，q来自decoder，kv来自encoder\n",
    "        output, attention_weight = self.attention(q, kv, kv, mask)\n",
    "\n",
    "        return output, attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84794c1",
   "metadata": {},
   "source": [
    "- **注意这里的 $q \\neq k = v$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fd608",
   "metadata": {},
   "source": [
    "## MHA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04e4a69",
   "metadata": {},
   "source": [
    "假设我们有 $h$ 个头，每个头拥有独立的线性变换矩阵 $W_i^Q, W_i^K, W_i^V$ (分别作用于查询、键和值的映射)，每个头的计算如下：\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n",
    "这些头的输出将沿最后一维拼接（Concat），并通过线性变换矩阵  $W^O$  映射回原始嵌入维度（embed_size）：\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$$\n",
    "其中 $h$ 是head_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10466f9",
   "metadata": {},
   "source": [
    "先从符合直觉的角度构造多头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11591515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-2): 3 x Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = 64\n",
    "num_heads = 3\n",
    "w_q = nn.ModuleList([nn.Linear(embed_size, embed_size) for _ in range(num_heads)])\n",
    "w_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82e207f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # 给每一个head单独定义QKV的线性层，输出维度为embed_size\n",
    "        self.w_q = nn.ModuleList(\n",
    "            [nn.Linear(embed_size, embed_size) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.w_k = nn.ModuleList(\n",
    "            [nn.Linear(embed_size, embed_size) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.w_v = nn.ModuleList(\n",
    "            [nn.Linear(embed_size, embed_size) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "        # 输出线性层，用于将多头拼接后的输出映射回 embed_size\n",
    "        self.fc_out = nn.Linear(num_heads * embed_size, embed_size)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        缩放点积注意力计算。\n",
    "        \n",
    "        参数:\n",
    "            Q: 查询矩阵 (batch_size, seq_len_q, embed_size)\n",
    "            K: 键矩阵 (batch_size, seq_len_k, embed_size)\n",
    "            V: 值矩阵 (batch_size, seq_len_v, embed_size)\n",
    "            mask: 掩码矩阵，用于屏蔽不应该关注的位置 (可选)\n",
    "\n",
    "        返回:\n",
    "            output: 注意力加权后的输出矩阵\n",
    "            attention_weights: 注意力权重矩阵\n",
    "        \"\"\"\n",
    "        # get d_k\n",
    "        embed_size = Q.size(-1)\n",
    "\n",
    "        # compute product / importance! divide sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(embed_size)\n",
    "\n",
    "        # 如果是带掩码的attention计算，掩码设置为负无穷\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "\n",
    "        # 在最后一维计算注意力分数, key value to probs\n",
    "        # seq_len_k == seq_len_v \n",
    "        attention_weight = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weight, V) \n",
    "        \n",
    "        return output, attention_weight \n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            q: 查询矩阵 (batch_size, seq_len_q, embed_size)\n",
    "            k: 键矩阵 (batch_size, seq_len_k, embed_size)\n",
    "            v: 值矩阵 (batch_size, seq_len_v, embed_size)\n",
    "            mask: 掩码矩阵 (batch_size, seq_len_q, seq_len_k)\n",
    "\n",
    "        返回:\n",
    "            out: 注意力加权后的输出\n",
    "            attention_weights: 注意力权重矩阵\n",
    "        \"\"\"\n",
    "        batch_size = q.shape[0]\n",
    "        multi_head_output = []\n",
    "        multi_head_weights = []\n",
    "        for i in range(self.num_heads):\n",
    "            Q = self.w_q[i](q)\n",
    "            K = self.w_k[i](k)\n",
    "            V = self.w_v[i](v)\n",
    "\n",
    "            # 计算注意力分数\n",
    "            scores, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "            multi_head_output.append(scores)\n",
    "            multi_head_weights.append(attn_weights)\n",
    "\n",
    "        # (B, S_q, embed_size) --concate-->  (B, S_q, num_heads * embed_size)\n",
    "        concat_out = torch.cat(multi_head_output, dim=-1)\n",
    "        output = self.fc_out(concat_out)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "319638f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MultiHeadAttention(\n",
       "   (w_q): ModuleList(\n",
       "     (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "   )\n",
       "   (w_k): ModuleList(\n",
       "     (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "   )\n",
       "   (w_v): ModuleList(\n",
       "     (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "   )\n",
       "   (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       " ),\n",
       " SingleHeadAttention(\n",
       "   (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       " ),\n",
       " MultiHeadAttention(\n",
       "   (w_q): ModuleList(\n",
       "     (0-7): 8 x Linear(in_features=512, out_features=512, bias=True)\n",
       "   )\n",
       "   (w_k): ModuleList(\n",
       "     (0-7): 8 x Linear(in_features=512, out_features=512, bias=True)\n",
       "   )\n",
       "   (w_v): ModuleList(\n",
       "     (0-7): 8 x Linear(in_features=512, out_features=512, bias=True)\n",
       "   )\n",
       "   (fc_out): Linear(in_features=4096, out_features=512, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiHeadAttention(512,1),SingleHeadAttention(512),MultiHeadAttention(512,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ab63a",
   "metadata": {},
   "source": [
    "Q: 现在所说的性能“提升”真的是由多头造成的吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cbb95b",
   "metadata": {},
   "source": [
    "![](./img/thinking%20deepth.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52beaa8",
   "metadata": {},
   "source": [
    "**实际上这种构造的方式使得参数量线性增长，并且在信息的拟合上可能存在overfit（每一个qkv的参数都是全空间的参数）**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f209d10",
   "metadata": {},
   "source": [
    "### True MHA 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bdcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        \"\"\"\"\n",
    "        MultiHeadAttention \n",
    "        args:\n",
    "            embed_size: 输入序列的嵌入维度\n",
    "            num_heads: 注意力头的数量\n",
    "            注意，head_dim = embed_size / num_heads(必须能够整除)\n",
    "        \"\"\"\n",
    "        super(MHA, self).__init__()\n",
    "        assert embed_size % num_heads == 0 , \"embed_size must divide by num_heads\"\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "\n",
    "        # 给每一个head单独定义QKV的线性层，输出维度为embed_size\n",
    "        self.w_q = nn.ModuleList(\n",
    "            [nn.Linear(embed_size, self.head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.w_k = nn.ModuleList(\n",
    "            [nn.Linear(embed_size, self.head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.w_v = nn.ModuleList(\n",
    "            [nn.Linear(embed_size, self.head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "        # 输出线性层，用于将多头拼接后的输出映射回 embed_size\n",
    "        self.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        缩放点积注意力计算。\n",
    "        \n",
    "        参数:\n",
    "            Q: 查询矩阵 (batch_size, seq_len_q, embed_size)\n",
    "            K: 键矩阵 (batch_size, seq_len_k, embed_size)\n",
    "            V: 值矩阵 (batch_size, seq_len_v, embed_size)\n",
    "            mask: 掩码矩阵，用于屏蔽不应该关注的位置 (可选)\n",
    "\n",
    "        返回:\n",
    "            output: 注意力加权后的输出矩阵\n",
    "            attention_weights: 注意力权重矩阵\n",
    "        \"\"\"\n",
    "        # get d_k\n",
    "        embed_size = Q.size(-1)\n",
    "\n",
    "        # compute product / importance! divide sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(embed_size)\n",
    "\n",
    "        # 如果是带掩码的attention计算，掩码设置为负无穷\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask==0, float('-inf'))\n",
    "\n",
    "        # 在最后一维计算注意力分数, key value to probs\n",
    "        # seq_len_k == seq_len_v \n",
    "        attention_weight = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weight, V) \n",
    "        \n",
    "        return output, attention_weight \n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            q: 查询矩阵 (batch_size, seq_len_q, head_dim)\n",
    "            k: 键矩阵 (batch_size, seq_len_k, head_dim)\n",
    "            v: 值矩阵 (batch_size, seq_len_v, head_dim)\n",
    "            mask: 掩码矩阵 (batch_size, seq_len_q, seq_len_k)\n",
    "\n",
    "        返回:\n",
    "            out: 注意力加权后的输出\n",
    "            attention_weights: 注意力权重矩阵\n",
    "        \"\"\"\n",
    "        batch_size = q.shape[0]\n",
    "        multi_head_output = []\n",
    "        multi_head_weights = []\n",
    "        for i in range(self.num_heads):\n",
    "            Q = self.w_q[i](q)\n",
    "            K = self.w_k[i](k)\n",
    "            V = self.w_v[i](v)\n",
    "\n",
    "            # 计算注意力分数\n",
    "            scores, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "            multi_head_output.append(scores)\n",
    "            multi_head_weights.append(attn_weights)\n",
    "\n",
    "        # (B, S_q, head_dim) --concate-->  (B, S_q, num_heads * head_dim)\n",
    "        concat_out = torch.cat(multi_head_output, dim=-1)\n",
    "        output = self.fc_out(concat_out)\n",
    "        return output    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae2e0f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MHA(\n",
       "  (w_q): ModuleList(\n",
       "    (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "  )\n",
       "  (w_k): ModuleList(\n",
       "    (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "  )\n",
       "  (w_v): ModuleList(\n",
       "    (0-7): 8 x Linear(in_features=512, out_features=64, bias=True)\n",
       "  )\n",
       "  (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MHA(512, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0208a265",
   "metadata": {},
   "source": [
    "虽然逻辑上很直观，但是运行速度极慢。接下来实现tranformers版本的MHA（矩阵运算）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5cebe5",
   "metadata": {},
   "source": [
    "![](./img/MHAinAttentionIsAllYourNeed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83522878",
   "metadata": {},
   "source": [
    "首先修改一下变量名：\n",
    "- `embed_size` $\\rightarrow$ $d_{model}$\n",
    "- `num_heads` $\\rightarrow$ h\n",
    "- `head_dim` $\\rightarrow$ $d_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe8f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional\n",
    "import math\n",
    "\n",
    "class TransformerMHA(nn.Module):\n",
    "    def __init__(self, d_model, h):\n",
    "        \"\"\"\n",
    "        多头注意力机制\n",
    "        args:\n",
    "            d_model:输入序列的维度大小\n",
    "            h：attention heads的数量 \n",
    "        \"\"\"\n",
    "        super(TransformerMHA, self).__init__()\n",
    "        assert d_model % h ==0, \"d_model must be divided by h (number of heads)\"\n",
    "\n",
    "        self.d_model = d_model \n",
    "        self.h = h\n",
    "\n",
    "        # QKV Linear layer\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        args: \n",
    "            i = q,k,v shape is (B, seq_len_{i}, d_model)\n",
    "            mask shape is (B, 1, seq_len_q, seq_len_k)\n",
    "            and seq_len_k = seq_len_v unequal to seq_len_q\n",
    "        return:\n",
    "            outputs:(weighted) shape is (batch_size, h, seq_len_q, seq_len_kv)\n",
    "            attntion_weights shape is (batch_size, h, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len_q, _ = q.size()\n",
    "        seq_len_kv, _ , _ = k.size()\n",
    "\n",
    "        # 将d_model 拆分为 head_numbers and head_dim \n",
    "        # 并且 (B, S_ , num_head, head_dim)  ---> (B , num_head, , S_, head_dim)\n",
    "        Q = self.w_q(q).view(batch_size, seq_len_q, self.h, -1).transpose(1,2)\n",
    "        K = self.w_k(q).view(batch_size, seq_len_kv, self.h, -1).transpose(1,2)\n",
    "        V = self.w_v(q).view(batch_size, seq_len_kv, self.h, -1).transpose(1,2)\n",
    "\n",
    "        scaled_attention, _ = scaled_dot_product_attention(Q, K, V, mask=None)\n",
    "\n",
    "        concat_out = scaled_attention.transpose(1,2).contiguous()\n",
    "        concat_out.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        out = self.fc_out(concat_out)\n",
    "        return out \n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        缩放点积注意力计算。\n",
    "        \n",
    "        args:\n",
    "            Q: 查询矩阵 (batch_size, num_heads, seq_len_q, d_k)\n",
    "            K: 键矩阵 (batch_size, num_heads, seq_len_kv, d_k)\n",
    "            V: 值矩阵 (batch_size, num_heads, seq_len_kv, d_v)\n",
    "            mask: 掩码矩阵 (batch_size, 1, seq_len_q, seq_len_kv) 或 \n",
    "            (1, 1, seq_len_q, seq_len_kv) 或 \n",
    "            (batch_size, h, seq_len_q, seq_len_kv)\n",
    "\n",
    "        return:\n",
    "            output: 注意力加权后的输出矩阵\n",
    "            attention_weights: 注意力权重矩阵\n",
    "        \"\"\"\n",
    "        d_k = Q.size(-1)  # d_k\n",
    "        \n",
    "        # 计算点积并进行缩放\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        # 如果提供了掩码矩阵，则将掩码对应位置的分数设为 -inf\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # 对缩放后的分数应用 Softmax 函数，得到注意力权重\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # 加权求和，计算输出\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3db6973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerMHA(\n",
       "  (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TransformerMHA(512, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7baa596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
