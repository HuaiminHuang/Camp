{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1457a0d2",
   "metadata": {},
   "source": [
    "# 序列标注任务\n",
    "- 目标： 为文本的每一个token分配一个标签。\n",
    "- 常见的序列标注任务有命名实体识别 NER (Named Entity Recognition) 和词性标注 POS (Part-Of-Speech tagging)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8ed1b1",
   "metadata": {},
   "source": [
    "```markdown\n",
    "我们选择 1998 年人民日报语料库作为数据集，该语料库标注了大量的语言学信息，可以同时用于分词、NER 等任务。这里我们直接使用处理好的 NER 语料 china-people-daily-ner-corpus.tar.gz。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef153693",
   "metadata": {},
   "source": [
    "## readme for People's Daily(人民日报) dataset\n",
    "### Task\n",
    "Named Entity Recognition\n",
    "### Description\n",
    "**Tags**: LOC(地名), ORG(机构名), PER(人名)   \n",
    "**Tag Strategy**：BIO  \n",
    "**Split**: '*space*' (北 B-LOC)  \n",
    "**Data Size**:  \n",
    "Train data set ( [example.train](example.train) ):  \n",
    "\n",
    "|句数|字符数|LOC数|ORG数|PER数|\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|20864|979180|16571|9277|8144|\n",
    "\n",
    "Dev data set ( [example.dev](example.dev) ):  \n",
    "\n",
    "|句数|字符数|LOC数|ORG数|PER数|\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|2318|109870|1951|984|884|\n",
    "\n",
    "Test data set ( [example.test](example.test) )\n",
    "\n",
    "|句数|字符数|LOC数|ORG数|PER数|\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|4636|219197|3658|2185|1864|\n",
    "\n",
    "**Reference**:   \n",
    "<https://github.com/zjy-ucas/ChineseNER>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431d73e",
   "metadata": {},
   "source": [
    "## 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4558118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "categories = set()\n",
    "\n",
    "class PeopleDaily(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "\n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open (data_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "            # 文本使用空行进行分割句子\n",
    "            for idx, line in enumerate(f.read().split(\"\\n\\n\")):\n",
    "                if not line:\n",
    "                    break\n",
    "                sentence, labels = \"\", []\n",
    "                for i, item in enumerate(line.split(\"\\n\")):\n",
    "                    char, tag = item.split(\" \")\n",
    "                    sentence += char\n",
    "                    if tag.startswith(\"B\"):\n",
    "                        labels.append([i, i, char, tag[2:]])   # Remove the B- or I-\n",
    "                        categories.add(tag[2:])\n",
    "                    elif tag.startswith(\"I\"):\n",
    "                        labels[-1][1] = i\n",
    "                        labels[-1][2] += char\n",
    "                Data[idx] = {\n",
    "                    \"sentence\" : sentence,\n",
    "                    \"labels\" : labels\n",
    "                }\n",
    "        return Data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04643ce0",
   "metadata": {},
   "source": [
    "```markdown\n",
    "中 B-LOC\n",
    "国 I-LOC\n",
    "人 O\n",
    "民 O\n",
    "银 B-ORG\n",
    "行 I-ORG```\n",
    "\n",
    "- 对应的输出为\n",
    "\n",
    "```python\n",
    "{\n",
    "    'sentence': '中国人民银行',\n",
    "    'labels': [\n",
    "        [0, 1, \"中国\", \"LOC\"],    # 位置0-1的\"中国\"是地点\n",
    "        [4, 5, \"银行\", \"ORG\"]     # 位置4-5的\"银行\"是组织机构\n",
    "    ]\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c7f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': '海钓比赛地点在厦门与金门之间的海域。', 'labels': [[7, 8, '厦门', 'LOC'], [10, 11, '金门', 'LOC']]} \n",
      " {'sentence': '这座依山傍水的博物馆由国内一流的设计师主持设计，整个建筑群精美而恢宏。', 'labels': []} \n",
      " {'PER', 'LOC', 'ORG'}\n"
     ]
    }
   ],
   "source": [
    "train_data = PeopleDaily('dataset/PeopleDaily/example.train')\n",
    "valid_data = PeopleDaily('dataset/PeopleDaily/example.dev')\n",
    "test_data = PeopleDaily('dataset/PeopleDaily/example.test')\n",
    "\n",
    "print(train_data[0], \"\\n\", train_data[1], \"\\n\", categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7213a162",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae5566",
   "metadata": {},
   "source": [
    "很容易的我们建立以下的标签mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50345ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-LOC', 2: 'I-LOC', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-PER', 6: 'I-PER'}\n",
      "{'O': 0, 'B-LOC': 1, 'I-LOC': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-PER': 5, 'I-PER': 6}\n"
     ]
    }
   ],
   "source": [
    "id2label = {0 : \"O\"}\n",
    "for l in list(sorted(categories)):\n",
    "    id2label[len(id2label)] = f\"B-{l}\"   # 使用当前字典长度作为新索引\n",
    "    id2label[len(id2label)] = f\"I-{l}\"   # 长度已增加，所以这是下一个索引\n",
    "label2id = {v : k for k,v in id2label.items()} # kv互换\n",
    "\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e70682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhm18\\miniconda3\\envs\\TrainingCamp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间', '的', '海', '域', '。', '[SEP]']\n",
      "[0 0 0 0 0 0 0 0 1 2 0 1 2 0 0 0 0 0 0 0]\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# 示例，把尸体标签转化为实体编号\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sentence = '海钓比赛地点在厦门与金门之间的海域。'\n",
    "labels = [[7, 8, '厦门', 'LOC'], [10, 11, '金门', 'LOC']]\n",
    "\n",
    "encoding = tokenizer(sentence, truncation=True)\n",
    "tokens = encoding.tokens()\n",
    "label = np.zeros(len(tokens), dtype=int)\n",
    "\n",
    "for char_start, char_end, word, tag in labels:\n",
    "\n",
    "    token_start = encoding.char_to_token(char_start)\n",
    "    token_end = encoding.char_to_token(char_end)\n",
    "\n",
    "    label[token_start] = label2id[f\"B-{tag}\"]\n",
    "    label[token_start +1 : token_end +1] = label2id[f\"I-{tag}\"]\n",
    "\n",
    "\n",
    "print(tokens)\n",
    "print(label)\n",
    "print([id2label[id] for id in label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a679e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def collote_fn(batch_samples):\n",
    "    batch_sentence, batch_tag = [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_sentence.append(sample[\"sentence\"])\n",
    "        batch_tag.append(sample[\"labels\"])\n",
    "    batch_inputs = tokenizer(\n",
    "        batch_sentence,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    batch_label = np.zeros(batch_inputs[\"input_ids\"].shape, dtype=int)\n",
    "    for s_idx, sentence in enumerate(batch_sentence):\n",
    "        encoding = tokenizer(sentence, truncation=True)\n",
    "\n",
    "        # 特殊标签[CLS] [SEP] ... 屏蔽\n",
    "        batch_label[s_idx][0] = -100\n",
    "        batch_label[s_idx][len(encoding.tokens())-1:] = -100\n",
    "\n",
    "        for char_start, char_end, _, tag in batch_tag[s_idx]:\n",
    "\n",
    "            token_start = encoding.char_to_token(char_start)\n",
    "            token_end = encoding.char_to_token(char_end)\n",
    "\n",
    "            batch_label[s_idx][token_start] = label2id[f\"B-{tag}\"]\n",
    "            batch_label[s_idx][token_start +1 : token_end +1] = label2id[f\"I-{tag}\"]\n",
    "    return batch_inputs, torch.tensor(batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a4c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=8, shuffle=False, collate_fn=collote_fn)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8, shuffle=False, collate_fn=collote_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec0d4cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([8, 56]), 'token_type_ids': torch.Size([8, 56]), 'attention_mask': torch.Size([8, 56])}\n",
      "batch_y shape: torch.Size([8, 56])\n",
      "{'input_ids': tensor([[ 101,  754, 3221, 8024, 1072, 3300, 1126, 1282, 2399,  837, 5320, 4638,\n",
      "         1921, 3823, 6639, 4413, 1762, 1059, 3173, 6225, 2573,  722,  678, 6672,\n",
      "          677, 3173, 4638, 6629, 6651, 5296,  511,  102,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 1745,  711, 1762, 6421, 1905, 1374, 2458, 4638, 2876, 2360, 3119,\n",
      "         2530, 5041, 5276,  811, 2466,  677, 8024, 2530, 2475,  812, 5314, 2360,\n",
      "          987, 2847,  677, 5273, 2506, 8024,  809, 6134, 2697, 6468,  722, 2658,\n",
      "          511,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 1744, 7354, 6639, 5468, 3173,  712, 2375, 2357, 2861, 4294, 2828,\n",
      "         2349, 2861, 1764, 6822, 1057, 1282, 1063, 2487, 4917,  868,  671,  702,\n",
      "          100, 2661, 6385,  100,  511,  102,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101,  679,  749, 6237, 6821,  671, 4157, 8024, 2218,  679, 5543, 1041,\n",
      "         1146, 4415, 6237, 6924, 2207, 2398, 2190, 4862, 1744, 5320,  671,  752,\n",
      "          689,  967, 3800, 4638, 2342, 1920, 4178, 2658, 8024, 2218,  679, 5543,\n",
      "         1041, 1146, 4415, 6237,  100,  671, 1744,  697, 1169,  100, 3354, 2682,\n",
      "         4638, 1158, 6863, 2595, 3975, 3787,  511,  102],\n",
      "        [ 101, 3683, 6612,  704, 8024, 5529, 5783, 1290, 2809, 5273,  809,  803,\n",
      "          782, 2900, 6662, 2458, 2229, 8024, 6627, 1744, 5783,  809, 1293, 2419,\n",
      "         4152, 2418, 2190, 8024, 2357, 2229, 7348, 3667,  697,  782, 3186, 7961,\n",
      "         4685, 2496,  511,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 6929,  720, 8024,  791, 1921, 1126, 2768, 1366, 1928, 4883, 4638,\n",
      "          100, 1377, 2589, 1921,  678, 4266, 3678, 2552,  100, 1348, 1963,  862,\n",
      "         6237, 6438, 8043,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 5011, 5442, 1762, 6444, 3389,  704,  749, 6237, 1168, 8024, 7357,\n",
      "         3378, 4638, 1036, 2094, 1762, 2458, 2497, 1008, 2372, 1139, 4909, 2421,\n",
      "         3198, 8024, 1728,  837, 3064, 3915, 4920, 2497, 1008,  754, 8432, 2399,\n",
      "         6158, 2496, 1765, 1062, 2128, 3322, 1068, 3389, 5815, 8024, 1400, 6158,\n",
      "         1161, 1152,  511,  102,    0,    0,    0,    0],\n",
      "        [ 101, 4506, 7313, 1765, 1928, 3291, 1914, 1765, 7306, 4385, 4708, 4906,\n",
      "         2825,  782, 1447, 1944, 2564, 4638, 6716, 2512,  511,  102,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            1,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    3,    4,    4,    4,    0,    0,    0,    5,    6,    6,    0,\n",
      "            3,    4,    4,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    5,    6,    6,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0, -100],\n",
      "        [-100,    0,    0,    0,    0,    5,    6,    6,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    5,    6,    6,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    5,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0, -100, -100, -100, -100, -100],\n",
      "        [-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_X.items()})\n",
    "print('batch_y shape:', batch_y.shape)\n",
    "print(batch_X)\n",
    "print(batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94bc72e",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55b6ccaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForNER were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForNER(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "class BertForNER(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(768, len(id2label))\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, x):\n",
    "        bert_output = self.bert(**x)\n",
    "        sequence_output = bert_output.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = BertForNER.from_pretrained(checkpoint, config=config).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2e7bb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 56, 7])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(batch_X.to(device))\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc47f47",
   "metadata": {},
   "source": [
    "优化模型参数：\n",
    "- 对于高维输出，交叉熵损失需要维度对齐 \n",
    "- (batch, seq_len, label_num) $\\rightarrow$ (batch, label_num, seq_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57e27ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader, start=1):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred.permute(0, 2, 1), y) # 交换维度\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_batch_num + batch):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195d7739",
   "metadata": {},
   "source": [
    "验证/测试循环负责评估模型的性能。这里我们借助 seqeval 库进行评估，seqeval 是一个专门用于序列标注评估的 Python 库，支持 IOB、IOB、IOBES 等多种标注格式以及多种评估策略，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef2ec149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.50      0.50      0.50         2\n",
      "         PER       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       0.67      0.67      0.67         3\n",
      "   macro avg       0.75      0.75      0.75         3\n",
      "weighted avg       0.67      0.67      0.67         3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seqeval\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "y_true = [['O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'B-LOC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "y_pred = [['O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'B-LOC', 'O'], ['B-PER', 'I-PER', 'O']]\n",
    "\n",
    "print(classification_report(y_true, y_pred,  mode='strict', scheme=IOB2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586fe13a",
   "metadata": {},
   "source": [
    "这里几个具体指标的含义：\n",
    "1. precision（精确率）\n",
    "   - 定义：\n",
    "     $$\n",
    "     \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "     $$\n",
    "   - TP（True Positive）：正确预测为该类的数量。\n",
    "   - FP（False Positive）：错误预测为该类的数量。\n",
    "   - 含义：在所有被预测为某个类别的实体中，有多少是真实正确的。\n",
    "\n",
    "2. recall（召回率）\n",
    "   - 定义：\n",
    "     $$\n",
    "     \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "     $$\n",
    "   - FN（False Negative）：真实属于该类，但预测成别的类的数量。\n",
    "   - 含义：在所有真实存在的某类实体中，模型能识别出来多少。\n",
    "   - 作用：衡量模型覆盖真实标签的能力。高召回率意味着漏报少（例如，大部分真实人名都被模型识别出来）。\n",
    "\n",
    "3. f1-score（F1 值）\n",
    "   - 定义：精确率和召回率的调和平均：\n",
    "     $$\n",
    "     \\text{F1} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "     $$\n",
    "   - 含义：平衡考虑了“少报”和“多报”两方面。\n",
    "   - 作用：当 precision 和 recall 需要综合考虑时，F1 是一个更公平的评价指标。对于 NER 这种要求同时“发现实体”又“分类正确”的任务，F1 是最常用指标。\n",
    "\n",
    "4. support\n",
    "   - 定义：数据集中该类别真实样本的数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097bd68",
   "metadata": {},
   "source": [
    "可以看到，对于第一个地点实体，模型虽然预测正确了其中 2 个 token 的标签，但是仍然判为识别错误，只有当预测的**起始和结束位置都正确时才算识别正确**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18759432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "from seqeval.scheme import IOB2\n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "    true_labels, true_predictions = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            predictions = pred.argmax(dim=-1).cpu().numpy().tolist()\n",
    "            labels = y.cpu().numpy().tolist()\n",
    "            true_labels += [[id2label[int(l)] for l in label if l != -100] for label in labels]\n",
    "            true_predictions += [\n",
    "                [id2label[int(p)] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "    print(classification_report(true_labels, true_predictions, mode='strict', scheme=IOB2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7965b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.PeopleDaily"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1657657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.000000:   0%|                                                                         | 0/2608 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.059594: 100%|██████████████████████████████████████████████████████████████| 2608/2608 [05:49<00:00,  7.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 290/290 [00:11<00:00, 25.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.94      0.95      0.94      1951\n",
      "         ORG       0.89      0.90      0.89       984\n",
      "         PER       0.97      0.98      0.97       884\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      3819\n",
      "   macro avg       0.93      0.94      0.94      3819\n",
      "weighted avg       0.94      0.94      0.94      3819\n",
      "\n",
      "Epoch 2/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.038600: 100%|██████████████████████████████████████████████████████████████| 2608/2608 [05:44<00:00,  7.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 290/290 [00:11<00:00, 25.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.97      0.96      0.96      1951\n",
      "         ORG       0.92      0.91      0.92       984\n",
      "         PER       0.98      0.98      0.98       884\n",
      "\n",
      "   micro avg       0.96      0.95      0.96      3819\n",
      "   macro avg       0.96      0.95      0.95      3819\n",
      "weighted avg       0.96      0.95      0.96      3819\n",
      "\n",
      "Epoch 3/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.028850: 100%|██████████████████████████████████████████████████████████████| 2608/2608 [05:53<00:00,  7.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 290/290 [00:14<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.97      0.96      0.97      1951\n",
      "         ORG       0.93      0.92      0.92       984\n",
      "         PER       0.99      0.98      0.98       884\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      3819\n",
      "   macro avg       0.96      0.95      0.96      3819\n",
      "weighted avg       0.96      0.96      0.96      3819\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "lr = 1e-5\n",
    "epoch = 3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch*len(train_dataloader)\n",
    ")\n",
    "\n",
    "total_loss = 0\n",
    "for t in range(epoch):\n",
    "    print(f\"Epoch {t+1}/{epoch}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, loss_fn, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    test_loop(valid_dataloader, model)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e96de",
   "metadata": {},
   "source": [
    "## 保存模型\n",
    "在实际应用中，我们会根据每一轮模型在验证集上的性能来调整超参数以及选出最好的权重，最后将选出的模型应用于测试集以评估最终的性能。因此，我们首先在上面的验证/测试循环中返回 seqeval 库计算出的指标，然后在每一个 Epoch 中根据 macro-F1/micro-F1 指标保存在验证集上最好的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0cce428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.006918: 100%|██████████████████████████████████████████████████████████████| 2608/2608 [06:13<00:00,  6.98it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 290/290 [00:12<00:00, 23.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.97      0.96      0.97      1951\n",
      "         ORG       0.93      0.92      0.92       984\n",
      "         PER       0.99      0.98      0.98       884\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      3819\n",
      "   macro avg       0.96      0.95      0.96      3819\n",
      "weighted avg       0.96      0.96      0.96      3819\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Epoch 2/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.006885: 100%|██████████████████████████████████████████████████████████████| 2608/2608 [06:22<00:00,  6.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 290/290 [00:12<00:00, 23.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.97      0.96      0.97      1951\n",
      "         ORG       0.93      0.92      0.92       984\n",
      "         PER       0.99      0.98      0.98       884\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      3819\n",
      "   macro avg       0.96      0.95      0.96      3819\n",
      "weighted avg       0.96      0.96      0.96      3819\n",
      "\n",
      "Epoch 3/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.006930: 100%|██████████████████████████████████████████████████████████████| 2608/2608 [05:55<00:00,  7.34it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 290/290 [00:11<00:00, 25.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.97      0.96      0.97      1951\n",
      "         ORG       0.93      0.92      0.92       984\n",
      "         PER       0.99      0.98      0.98       884\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      3819\n",
      "   macro avg       0.96      0.95      0.96      3819\n",
      "weighted avg       0.96      0.96      0.96      3819\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def test_loop(dataloader, model):\n",
    "    true_labels, true_predictions = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in tqdm(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            predictions = pred.argmax(dim=-1).cpu().numpy().tolist()\n",
    "            labels = y.cpu().numpy().tolist()\n",
    "            true_labels += [[id2label[int(l)] for l in label if l != -100] for label in labels]\n",
    "            true_predictions += [\n",
    "                [id2label[int(p)] for (p, l) in zip(prediction, label) if l != -100]\n",
    "                for prediction, label in zip(predictions, labels)\n",
    "            ]\n",
    "    print(classification_report(true_labels, true_predictions, mode='strict', scheme=IOB2))\n",
    "    return classification_report(\n",
    "      true_labels, \n",
    "      true_predictions, \n",
    "      mode='strict', \n",
    "      scheme=IOB2, \n",
    "      output_dict=True\n",
    "    )\n",
    "\n",
    "total_loss = 0.\n",
    "best_f1 = 0.\n",
    "for t in range(epoch):\n",
    "    print(f\"Epoch {t+1}/{epoch}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, loss_fn, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    metrics = test_loop(valid_dataloader, model)\n",
    "    valid_macro_f1, valid_micro_f1 = metrics['macro avg']['f1-score'], metrics['micro avg']['f1-score']\n",
    "    valid_f1 = metrics['weighted avg']['f1-score']\n",
    "    if valid_f1 > best_f1:\n",
    "        best_f1 = valid_f1\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(\n",
    "            model.state_dict(), \n",
    "            f'epoch_{t+1}_valid_macrof1_{(100*valid_macro_f1):0.3f}_microf1_{(100*valid_micro_f1):0.3f}_weights.bin'\n",
    "        )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3225db",
   "metadata": {},
   "source": [
    "## 测试模型 & 保存预测结果\n",
    "模型的输出是一个由预测向量组成的列表，每个向量对应一个 token 的预测结果，只需要在输出 logits 值上运用 softmax 函数就可以获得实体类别的预测概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bfbab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhm18\\AppData\\Local\\Temp\\ipykernel_3360\\316242649.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load('./model/epoch_1_valid_macrof1_95.786_microf1_95.912_weights.bin', map_location=torch.device(device))\n"
     ]
    }
   ],
   "source": [
    "sentence = '日本外务省3月18日发布消息称，日本首相岸田文雄将于19至21日访问印度和柬埔寨。'\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load('./model/epoch_1_valid_macrof1_95.786_microf1_95.912_weights.bin', map_location=torch.device(device))\n",
    ")\n",
    "model.eval()\n",
    "results = []\n",
    "with torch.no_grad():\n",
    "    # offset_mapping（只有 fast tokenizer 且 return_offsets_mapping=True 时才有）。\n",
    "    # offset_mapping 的形状通常是 (1, seq_len, 2)，\n",
    "    # 每个 token 对应一个 (start_char, end_char) 的元组（字符级索引，基于原始 sentence）。\n",
    "    inputs = tokenizer(sentence, truncation=True, return_tensors=\"pt\", \n",
    "                       return_offsets_mapping=True)\n",
    "    offsets = inputs.pop('offset_mapping').squeeze(0)\n",
    "    inputs = inputs.to(device)\n",
    "    pred = model(inputs)\n",
    "    probabilities = torch.nn.functional.softmax(pred, dim=-1)[0].cpu().numpy().tolist()\n",
    "    predictions = pred.argmax(dim=-1)[0].cpu().numpy().tolist()\n",
    "\n",
    "    pred_label = []\n",
    "    idx = 0\n",
    "    while idx < len(predictions):\n",
    "        pred = predictions[idx]\n",
    "        label = id2label[pred]\n",
    "        if label != \"O\":\n",
    "            label = label[2:] # Remove the B- or I-\n",
    "            start, end = offsets[idx]\n",
    "            all_scores = [probabilities[idx][pred]]\n",
    "            # Grab all the tokens labeled with I-label\n",
    "            while (\n",
    "                idx + 1 < len(predictions) and \n",
    "                id2label[predictions[idx + 1]] == f\"I-{label}\"\n",
    "            ):\n",
    "                all_scores.append(probabilities[idx + 1][predictions[idx + 1]])\n",
    "                _, end = offsets[idx + 1]\n",
    "                idx += 1\n",
    "\n",
    "            score = np.mean(all_scores).item()\n",
    "            start, end = start.item(), end.item()\n",
    "            word = sentence[start:end]\n",
    "            pred_label.append(\n",
    "                {\n",
    "                    \"entity_group\": label,\n",
    "                    \"score\": score,\n",
    "                    \"word\": word,\n",
    "                    \"start\": start,\n",
    "                    \"end\": end,\n",
    "                }\n",
    "            )\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bc1aef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.9992446899414062,\n",
       "  'word': '日本外务省',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9975488781929016,\n",
       "  'word': '日本',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.9988918155431747,\n",
       "  'word': '岸田文雄',\n",
       "  'start': 20,\n",
       "  'end': 24},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9993538856506348,\n",
       "  'word': '印度',\n",
       "  'start': 34,\n",
       "  'end': 36},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9988286892573038,\n",
       "  'word': '柬埔寨',\n",
       "  'start': 37,\n",
       "  'end': 40}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09ef211",
   "metadata": {},
   "source": [
    "还可以扩展上面的代码进行数据集（测试集）的处理，并把预测的结果保存在json格式的文件里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d245c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhm18\\AppData\\Local\\Temp\\ipykernel_3360\\3582947573.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load('./model/epoch_1_valid_macrof1_95.786_microf1_95.912_weights.bin', map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 580/580 [00:23<00:00, 24.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.95      0.95      0.95      3658\n",
      "         ORG       0.91      0.91      0.91      2185\n",
      "         PER       0.98      0.98      0.98      1864\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      7707\n",
      "   macro avg       0.95      0.95      0.95      7707\n",
      "weighted avg       0.95      0.95      0.95      7707\n",
      "\n",
      "predicting labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4636/4636 [02:14<00:00, 34.36it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load('./model/epoch_1_valid_macrof1_95.786_microf1_95.912_weights.bin', map_location=torch.device('cpu'))\n",
    ")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('evaluating on test set...')\n",
    "    true_labels, true_predictions = [], []\n",
    "    for X, y in tqdm(test_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        predictions = pred.argmax(dim=-1).cpu().numpy().tolist()\n",
    "        labels = y.cpu().numpy().tolist()\n",
    "        true_labels += [[id2label[int(l)] for l in label if l != -100] for label in labels]\n",
    "        true_predictions += [\n",
    "            [id2label[int(p)] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "    print(classification_report(true_labels, true_predictions, mode='strict', scheme=IOB2))\n",
    "    results = []\n",
    "    print('predicting labels...')\n",
    "    for s_idx in tqdm(range(len(test_data))):\n",
    "        example = test_data[s_idx]\n",
    "        inputs = tokenizer(example['sentence'], truncation=True, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(device)\n",
    "        pred = model(inputs)\n",
    "        probabilities = torch.nn.functional.softmax(pred, dim=-1)[0].cpu().numpy().tolist()\n",
    "        predictions = pred.argmax(dim=-1)[0].cpu().numpy().tolist()\n",
    "\n",
    "        pred_label = []\n",
    "        inputs_with_offsets = tokenizer(example['sentence'], return_offsets_mapping=True)\n",
    "        tokens = inputs_with_offsets.tokens()\n",
    "        offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "        idx = 0\n",
    "        while idx < len(predictions):\n",
    "            pred = predictions[idx]\n",
    "            label = id2label[pred]\n",
    "            if label != \"O\":\n",
    "                label = label[2:] # Remove the B- or I-\n",
    "                start, end = offsets[idx]\n",
    "                all_scores = [probabilities[idx][pred]]\n",
    "                # Grab all the tokens labeled with I-label\n",
    "                while (\n",
    "                    idx + 1 < len(predictions) and \n",
    "                    id2label[predictions[idx + 1]] == f\"I-{label}\"\n",
    "                ):\n",
    "                    all_scores.append(probabilities[idx + 1][predictions[idx + 1]])\n",
    "                    _, end = offsets[idx + 1]\n",
    "                    idx += 1\n",
    "\n",
    "                score = np.mean(all_scores).item()\n",
    "                word = example['sentence'][start:end]\n",
    "                pred_label.append(\n",
    "                    {\n",
    "                        \"entity_group\": label,\n",
    "                        \"score\": score,\n",
    "                        \"word\": word,\n",
    "                        \"start\": start,\n",
    "                        \"end\": end,\n",
    "                    }\n",
    "                )\n",
    "            idx += 1\n",
    "        results.append(\n",
    "            {\n",
    "                \"sentence\": example['sentence'], \n",
    "                \"pred_label\": pred_label, \n",
    "                \"true_label\": example['labels']\n",
    "            }\n",
    "        )\n",
    "    with open('test_data_pred.json', 'wt', encoding='utf-8') as f:\n",
    "        for exapmle_result in results:\n",
    "            f.write(json.dumps(exapmle_result, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63891f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"json\", data_files=\"./dataset/test_data_pred.json\", split=\"train\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e65e00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"./dataset/test_data_pred.json\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6e9a055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '李瑞环向莫诺里介绍了中国人民政协的情况。',\n",
       " 'pred_label': [{'entity_group': 'PER',\n",
       "   'score': 0.9997104406356812,\n",
       "   'word': '李瑞环',\n",
       "   'start': 0,\n",
       "   'end': 3},\n",
       "  {'entity_group': 'PER',\n",
       "   'score': 0.999658465385437,\n",
       "   'word': '莫诺里',\n",
       "   'start': 4,\n",
       "   'end': 7},\n",
       "  {'entity_group': 'ORG',\n",
       "   'score': 0.9993860423564911,\n",
       "   'word': '中国人民政协',\n",
       "   'start': 10,\n",
       "   'end': 16}],\n",
       " 'true_label': [[0, 2, '李瑞环', 'PER'],\n",
       "  [4, 6, '莫诺里', 'PER'],\n",
       "  [10, 15, '中国人民政协', 'ORG']]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1082]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
