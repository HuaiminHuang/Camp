{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d82f0a",
   "metadata": {},
   "source": [
    "# Transformer 组件 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce4b89",
   "metadata": {},
   "source": [
    "## FFN（前馈神经网络）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c10d7d",
   "metadata": {},
   "source": [
    "![FFN](./img/pwFFN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8227e8",
   "metadata": {},
   "source": [
    "Position-wise 实际是线性层本身的一个特性，在线性层中，每个输入向量（对应于序列中的一个位置，比如一个词向量）都会通过相同的权重矩阵进行线性变换，这意味着每个位置的处理是相互独立的，逐元素这一点可以看成 kernal_size=1 的卷积核扫过一遍序列。\n",
    "\n",
    "FFN实现很简单，本质上是proj_up换加上激活函数，再加上一个proj_down\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8fd0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bacdf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        FFN\n",
    "        args:\n",
    "            d_model: 输入和输出向量的维度\n",
    "            d_ff： FFN隐藏层的维度\n",
    "            dropout：随机屏蔽部分输出，防止过拟合（也是一种正则化手段）\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.proj_up = nn.Linear(d_model, d_ff)\n",
    "        self.proj_down = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj_up(x).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.proj_down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15d45fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([64, 256, 512]) \n",
      "ffn(x) shape: torch.Size([64, 256, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_len = 256\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "print(\"x shape:\", x.shape, \"\\nffn(x) shape:\", ffn(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54008445",
   "metadata": {},
   "source": [
    "## 残差连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d04d34",
   "metadata": {},
   "source": [
    "残差连接是一种跳跃连接，将输入直接加入到输出上（实际上，有了残差连接后参数的更新只需要去做f(x)-x的部分即可？）：\n",
    "$$\\text{Output} = \\text{Sublayers}(x) + x$$\n",
    "主要作用是**缓解梯度消失/爆炸**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        \"\"\"\n",
    "        residual，用于在每个子层后添加残差连接和 Dropout。\n",
    "        \n",
    "        args:\n",
    "            dropout: 防止过拟合。\n",
    "        \"\"\"\n",
    "        super(ResidualConnection, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            x: 残差连接的输入张量，形状为 (batch_size, seq_len, d_model)。\n",
    "            sublayer: 子层模块的函数，多头注意力或前馈网络。\n",
    "\n",
    "        return:\n",
    "            经过残差连接和 Dropout 处理后的张量，形状为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        # 将子层输出应用 dropout，然后与输入相加（参见论文 5.4 的表述或者本文「呈现」部分）\n",
    "        return x + self.dropout(sublayer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fdf2bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([64, 256, 512]) \n",
      "ffn(x) and residual shape: torch.Size([64, 256, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_len = 256\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "residual = ResidualConnection()\n",
    "print(\"x shape:\", x.shape, \"\\nffn(x) and residual shape:\", residual(x, ffn).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b26d0",
   "metadata": {},
   "source": [
    "## Ebedding Layer（嵌入层）\n",
    "因为 token ID 只是整数标识符，彼此之间没有内在联系。如果直接使用这些整数，模型可能在训练过程中学习到一些模式，但无法充分捕捉词汇之间的语义关系，这显然不足以支撑起现在的大模型。\n",
    "- 作用：\n",
    "    - 捕捉语义信息（映射到高维的连续向量空间）\n",
    "    - embeding 类似于表格查找的操作\n",
    "- 参数可训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ea8d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    token ID tranform to embedding vector\n",
    "\n",
    "    args: \n",
    "        vocab_size: 词表大小\n",
    "        d_model：嵌入向量维度（隐藏层维度）\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.embed = nn.Linear(vocab_size, d_model)\n",
    "        self.scaled_factor = math.sqrt(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x_embed = x * self.scaled_factor\n",
    "        return x_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae554e68",
   "metadata": {},
   "source": [
    "## softmax\n",
    "将向量转化为**概率分布**\n",
    "$$\\text{Softmax(x)}_i =  \\frac{\\exp{(x_i)}}{\\sum_j {\\exp{(x_j)}}}$$\n",
    "指数形式放大变换前后的差异性，并且保证非负性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c951993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(x):\n",
    "    exp_x = torch.exp(x)\n",
    "    sum_exp_x = torch.sum(exp_x, dim=-1, keepdim=True)\n",
    "    return exp_x / sum_exp_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87064eaf",
   "metadata": {},
   "source": [
    "keepdim 保持维度，便于广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e60470dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[0.1442, 0.6782, 0.8527, 0.3439, 0.2320],\n",
      "        [0.3960, 0.2310, 0.7250, 0.1348, 0.8522]]) \n",
      "after softmax: tensor([[0.1419, 0.2420, 0.2881, 0.1732, 0.1549],\n",
      "        [0.1790, 0.1518, 0.2488, 0.1379, 0.2825]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 5)\n",
    "softmax_x = Softmax(x)\n",
    "print(\"x:\", x, \"\\nafter softmax:\", softmax_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdc44489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.],\n",
      "        [12.]])\n",
      "torch.Size([2, 1])\n",
      "tensor([ 3., 12.])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "y = torch.arange(6, dtype=torch.float32).reshape(2, 3)\n",
    "sum_keep = torch.sum(y, dim=-1, keepdim=True)\n",
    "print(sum_keep)        # tensor([[ 6.], [15.]])\n",
    "print(sum_keep.shape)  # torch.Size([2, 1])\n",
    "\n",
    "sum_no_keep = torch.sum(y, dim=-1)\n",
    "print(sum_no_keep)        # tensor([ 6., 15.])\n",
    "print(sum_no_keep.shape)  # torch.Size([2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca45d0",
   "metadata": {},
   "source": [
    "## CrossEntropy（交叉熵损失）\n",
    "公式：\n",
    "$$\\mathcal{L} = - \\sum_{i} y_i \\log(\\hat{y}_i) $$\n",
    "其中$y_i$是真实标签的one-hot编码，$\\hat{y}_i$是预测值\n",
    "\n",
    "- nn.CrossEntropyLoss()直接接受logits(没有经过softmax)\n",
    "\n",
    "```\n",
    "CrossEntropyLoss(logits, target) = NLLLoss(log_softmax(logits), target)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd2d23f",
   "metadata": {},
   "source": [
    "## PositionEbedding\n",
    "![](./img/PE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e05631",
   "metadata": {},
   "source": [
    "在原始论文中，Transformer 使用的是固定位置编码（Positional Encoding），其公式如下：\n",
    "\n",
    "\\begin{array}{c}\n",
    "P E_{(p o s, 2 i)}=\\sin \\left(\\frac{p o s}{10000^{2 i / d_{\\text {model }}}}\\right) \\\\\n",
    "P E_{(p o s, 2 i+1)}=\\cos \\left(\\frac{p o s}{10000^{2 i / d_{\\text {model }}}}\\right)\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "其中：\n",
    "- pos 表示位置索引（Position）。\n",
    "-  i  表示维度索引。\n",
    "-  $d_{\\text {model }}$  是嵌入向量的维度。\n",
    "\n",
    "![drpopout](./img/AboutDropout.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeding(nn.Module):\n",
    "    def __init__(self,d_model, dropout =0.1, max_len=5000):\n",
    "        \"\"\"\n",
    "        PE,添加序列的唯一位置信息\n",
    "\n",
    "        args:\n",
    "            d_model: 嵌入维度\n",
    "            dropout: 应用PE后的 Dropout的概率\n",
    "            max_len: 位置编码的最大长，适应不同的输入序列\n",
    "        \"\"\"\n",
    "        super(PositionEmbeding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)  # 论文的5.4 Residual Dropout\n",
    "        \n",
    "        # 创建位置编码矩阵 shape is (max_len, d_model)\n",
    "        # 位置索引为 (max_len, 1)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1) # unsqueeze(1) to broadcast d_model\n",
    "\n",
    "        # 频率计算\n",
    "        # pos / (10000^(2i/d_model)) \n",
    "        # = pos * exp(log(10000^(-2i/d_model)))\n",
    "        # = pos * exp(-2i/d_model * log(10000))\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model )\n",
    "        )\n",
    "\n",
    "        # 计算 sin 和 cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "\n",
    "        # 扩充维度广播 input (batch_size, seq_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # 将位置编码注册为模型的缓冲区，不作为参数更新\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]  # 去除相同序列长度的位置编码进行合并\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d0aa14",
   "metadata": {},
   "source": [
    "示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b16bab05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6715, 0.7376, 0.1806, 0.1719],\n",
      "         [0.7500, 0.3433, 0.6341, 0.1776],\n",
      "         [0.6561, 0.3824, 0.3398, 0.5301]],\n",
      "\n",
      "        [[0.8644, 0.7777, 0.3688, 0.9314],\n",
      "         [0.0840, 0.0080, 0.5564, 0.0306],\n",
      "         [0.7182, 0.2303, 0.6448, 0.1608]],\n",
      "\n",
      "        [[0.7516, 0.7898, 0.6295, 0.4097],\n",
      "         [0.5418, 0.5777, 0.6803, 0.8286],\n",
      "         [0.8426, 0.0279, 0.4214, 0.3846]],\n",
      "\n",
      "        [[0.4352, 0.7577, 0.0216, 0.4315],\n",
      "         [0.0411, 0.4526, 0.4413, 0.4428],\n",
      "         [0.5427, 0.2797, 0.2008, 0.6624]]]) \n",
      " tensor([[[ 0.7461,  1.9307,  0.2006,  1.3021],\n",
      "         [ 1.7683,  0.9817,  0.7156,  1.3084],\n",
      "         [ 0.0000, -0.0000,  0.3997,  1.6999]],\n",
      "\n",
      "        [[ 0.9604,  1.9752,  0.4098,  2.1460],\n",
      "         [ 1.0283,  0.6093,  0.6293,  1.1450],\n",
      "         [ 1.8083, -0.2065,  0.7387,  1.2896]],\n",
      "\n",
      "        [[ 0.8351,  0.0000,  0.6995,  1.5663],\n",
      "         [ 1.5369,  0.0000,  0.7670,  2.0318],\n",
      "         [ 1.9465, -0.4314,  0.4904,  0.0000]],\n",
      "\n",
      "        [[ 0.4836,  1.9531,  0.0240,  1.5905],\n",
      "         [ 0.9806,  1.1032,  0.5014,  1.6030],\n",
      "         [ 1.6133, -0.1516,  0.0000,  1.8469]]])\n"
     ]
    }
   ],
   "source": [
    "PE = PositionEmbeding(d_model=4)\n",
    "x = torch.rand(4,3,4)\n",
    "x_pe = PE(x)\n",
    "print(x, \"\\n\", x_pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4b457f",
   "metadata": {},
   "source": [
    "- encoder part\n",
    "    - 编码器的输入由输入嵌入（Input Embedding）和位置编码（Positional Encoding）组成，在机器翻译任务中，还可以称为源语言嵌入（Source Embedding）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91e5e353",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, src_vocab_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        inputs 部分的 embeddings\n",
    "\n",
    "        args:\n",
    "            scr_vocab_size: 源语言词汇表大小\n",
    "            d_model: 嵌入向量维度\n",
    "            dropout概率 \n",
    "        \"\"\"\n",
    "        super(SourceEmbedding, self).__init__()\n",
    "        self.embed = Embeddings(src_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionEmbeding(d_model, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(self.embed(x))  # (batch_size, seq_len_src, d_model)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc9b9a",
   "metadata": {},
   "source": [
    "- decoder part\n",
    "    - 解码器的输入由输出嵌入（Output Embedding）和位置编码（Positional Encoding）组成，在机器翻译这个任务中也可以称为目标语言嵌入（Target Embedding），为了避免与最终输出混淆，使用 TargetEmbedding 进行实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a9cb415",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, tgt_vocab_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        inputs 部分的 embeddings\n",
    "\n",
    "        args:\n",
    "            tgt_vocab_size: 源语言词汇表大小\n",
    "            d_model: 嵌入向量维度\n",
    "            dropout概率 \n",
    "        \"\"\"\n",
    "        super(SourceEmbedding, self).__init__()\n",
    "        self.embed = Embeddings(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionEmbeding(d_model, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(self.embed(x))  # (batch_size, seq_len_tgt, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23218022",
   "metadata": {},
   "source": [
    "### 掩码mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80850a69",
   "metadata": {},
   "source": [
    "1. Padding Mask\n",
    "- 填充掩码用于在注意力计算时屏蔽填充 \\<PAD\\> 位置，防止模型计算注意力权重的时候考虑这些无意义的位置，\n",
    "    - 注意：这里接受的参数为 pad_token_id，这意味着掩码操作在嵌入操作前，也就是分词（tokenize）然后映射为 Token IDs 后进行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2346afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq, pad_token_id=0):\n",
    "    # seq shape is [batch_size, seq_len] --> [batch_size, 1, 1, seq_len]\n",
    "    mask = (seq != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aff42b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ True,  True,  True, False, False]]],\n",
       " \n",
       " \n",
       "         [[[ True,  True, False, False, False]]]]),\n",
       " torch.Size([2, 1, 1, 5]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 示例 0 表示 <PAD>\n",
    "seq = torch.tensor([[5, 7, 9, 0, 0], [8, 6, 0, 0, 0]])  \n",
    "seq_masked = create_padding_mask(seq)\n",
    "seq_masked, seq_masked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b40dc30",
   "metadata": {},
   "source": [
    "2. Look-ahead Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df4673",
   "metadata": {},
   "source": [
    "防止模型训练时候偷看答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "766923a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.tril(torch.ones(size, size)).type(torch.bool)  # 下三角矩阵\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6723c51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False, False, False, False],\n",
      "        [ True,  True, False, False, False, False],\n",
      "        [ True,  True,  True, False, False, False],\n",
      "        [ True,  True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True,  True]]) torch.Size([6, 6])\n"
     ]
    }
   ],
   "source": [
    "print(create_look_ahead_mask(6), create_look_ahead_mask(6).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a85d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder_mask(tgt_seq, pad_token_id=0):\n",
    "    # (batch_size, 1, 1, seq_len_tgt)\n",
    "    padding_mask = create_padding_mask(tgt_seq, pad_token_id)\n",
    "\n",
    "     # (seq_len_tgt, seq_len_tgt)  \n",
    "    look_ahead_mask = create_look_ahead_mask(tgt_seq.size(1)).to(tgt_seq.device) \n",
    "\n",
    "    # broadcast to (batch_size, 1, seq_len_tgt, seq_len_tgt)\n",
    "    combined_mask = look_ahead_mask.unsqueeze(0) & padding_mask  \n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc00b849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ True, False, False, False, False, False, False],\n",
      "          [ True,  True, False, False, False, False, False],\n",
      "          [ True,  True,  True, False, False, False, False],\n",
      "          [ True,  True,  True,  True, False, False, False],\n",
      "          [ True,  True,  True,  True, False, False, False],\n",
      "          [ True,  True,  True,  True, False, False, False],\n",
      "          [ True,  True,  True,  True, False, False, False]]]])\n"
     ]
    }
   ],
   "source": [
    "tgt_seq = torch.tensor([[1, 2, 3, 4, 0, 0, 0]])  # 0 表示 <PAD>\n",
    "print(create_decoder_mask(tgt_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf25aae",
   "metadata": {},
   "source": [
    "### 总结\n",
    "\n",
    "假设目标序列 `tgt_seq = [A, B, C, D, <PAD>]`。\n",
    "\n",
    "- **填充掩码**：会屏蔽 `<PAD>` 位置。\n",
    "- **未来信息掩码**：在位置 `C`，模型只能看到 `A`、`B` 和 `C`，但在最后一个位置上，会看到全部（包括 `<PAD>`）。\n",
    "- **组合掩码**：在位置 `C`，模型只能看到 `A`、`B` 和 `C`，在最后一个位置上，模型也只能看到 `A`、`B`、`C` 和 `D`。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
