{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d82f0a",
   "metadata": {},
   "source": [
    "# Transformer 组件 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce4b89",
   "metadata": {},
   "source": [
    "## FFN（前馈神经网络）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c10d7d",
   "metadata": {},
   "source": [
    "![FFN](./img/pwFFN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8227e8",
   "metadata": {},
   "source": [
    "Position-wise 实际是线性层本身的一个特性，在线性层中，每个输入向量（对应于序列中的一个位置，比如一个词向量）都会通过相同的权重矩阵进行线性变换，这意味着每个位置的处理是相互独立的，逐元素这一点可以看成 kernal_size=1 的卷积核扫过一遍序列。\n",
    "\n",
    "FFN实现很简单，本质上是proj_up换加上激活函数，再加上一个proj_down\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8fd0285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bacdf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        FFN\n",
    "        args:\n",
    "            d_model: 输入和输出向量的维度\n",
    "            d_ff： FFN隐藏层的维度\n",
    "            dropout：随机屏蔽部分输出，防止过拟合（也是一种正则化手段）\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.proj_up = nn.Linear(d_model, d_ff)\n",
    "        self.proj_down = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj_up(x).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.proj_down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c15d45fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([64, 256, 512]) \n",
      "ffn(x) shape: torch.Size([64, 256, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_len = 256\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "print(\"x shape:\", x.shape, \"\\nffn(x) shape:\", ffn(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54008445",
   "metadata": {},
   "source": [
    "## 残差连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d04d34",
   "metadata": {},
   "source": [
    "残差连接是一种跳跃连接，将输入直接加入到输出上（实际上，有了残差连接后参数的更新只需要去做f(x)-x的部分即可？）：\n",
    "$$\\text{Output} = \\text{Sublayers}(x) + x$$\n",
    "主要作用是**缓解梯度消失/爆炸**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5e7e946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        \"\"\"\n",
    "        residual，用于在每个子层后添加残差连接和 Dropout。\n",
    "        \n",
    "        args:\n",
    "            dropout: 防止过拟合。\n",
    "        \"\"\"\n",
    "        super(ResidualConnection, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            x: 残差连接的输入张量，形状为 (batch_size, seq_len, d_model)。\n",
    "            sublayer: 子层模块的函数，多头注意力或前馈网络。\n",
    "\n",
    "        return:\n",
    "            经过残差连接和 Dropout 处理后的张量，形状为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        # 将子层输出应用 dropout，然后与输入相加（参见论文 5.4 的表述或者本文「呈现」部分）\n",
    "        return x + self.dropout(sublayer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fdf2bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([64, 256, 512]) \n",
      "ffn(x) and residual shape: torch.Size([64, 256, 512])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "seq_len = 256\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "residual = ResidualConnection()\n",
    "print(\"x shape:\", x.shape, \"\\nffn(x) and residual shape:\", residual(x, ffn).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b26d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
