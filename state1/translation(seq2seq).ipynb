{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a64d34",
   "metadata": {},
   "source": [
    "# 翻译任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcdc1a4",
   "metadata": {},
   "source": [
    "翻译经典的为序列到序列的任务（seq2seq）,在翻译形式上和其他的很多任务接近：\n",
    "- 文本摘要 (Summarization)：将长文本压缩为短文本，并且还要尽可能保留核心内容。\n",
    "- 风格转换 (Style transfer)：将文本转换为另一种书写风格，例如将文言文转换为白话文、将古典英语转换为现代英语；\n",
    "- 生成式问答 (Generative question answering)：对于给定的问题，基于上下文生成对应的答案。\n",
    "\n",
    "理论上我们也可以将这一节的操作应用于完成这些 Seq2Seq 任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44fff6",
   "metadata": {},
   "source": [
    "本章我们将微调一个 Marian 翻译模型进行汉英翻译，该模型已经基于 Opus 语料对汉英翻译任务进行了预训练，因此可以直接用于翻译。而通过我们的微调，可以进一步提升该模型在特定语料上的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2c3b5",
   "metadata": {},
   "source": [
    "## 数据集的准备\n",
    " translation2019zh 语料\n",
    " \n",
    " example：`{\"english\": \"In Italy, there is no real public pressure for a new, fairer tax system.\", \"chinese\": \"在意大利，公众不会真的向政府施压，要求实行新的、更公平的税收制度。\"}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed22577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "import json\n",
    "\n",
    "# 选择22w条数据，训练20w， 验证2w\n",
    "max_dataset_size = 220000\n",
    "train_set_size = 200000\n",
    "valid_set_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e28c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRANS(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "\n",
    "    def load_data(self, data_file):\n",
    "        Data = {}\n",
    "        with open(data_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if idx >= max_dataset_size:\n",
    "                    break\n",
    "                sample = json.loads(line.strip())\n",
    "                Data[idx] = sample\n",
    "        return Data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7dc797c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cd8caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TRANS(\"./dataset/translation2019zh/translation2019zh_train.json\")\n",
    "train_data, val_data = random_split(data, [train_set_size, valid_set_size])\n",
    "test_data = TRANS(\"./dataset/translation2019zh/translation2019zh_valid.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c74ed1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size: 200000\n",
      "valid set size: 20000\n",
      "test set size: 39323\n",
      "{'english': 'If you need a good facial scrub, you can coarsely grind some coffee beans and use them to scrub your face. They have great exfoliating properties.', 'chinese': '如果你需要优质的洁面粉，你可以粗粗磨一些咖啡豆，然后用它们打磨你的面部，能很好地去角质噢。'}\n"
     ]
    }
   ],
   "source": [
    "print(f'train set size: {len(train_data)}')\n",
    "print(f'valid set size: {len(val_data)}')\n",
    "print(f'test set size: {len(test_data)}')\n",
    "print(next(iter(train_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacea0ed",
   "metadata": {},
   "source": [
    "## 数据预处理\n",
    "接下来我们通过选择 Helsinki-NLP 提供的汉英翻译模型 opus-mt-zh-en 对应的分词器进行 token IDs的转化\n",
    "- model_checkpoint 设置为对应的语言即可\n",
    "- 默认情况下分词器会采用源语言的设定来编码文本（对于英翻译模型 opus-mt-zh-en 而言就是中文），要编码目标语言则需要使用 text_targets 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4c563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/state1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/miniconda3/envs/state1/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"./model/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9d77795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecea247",
   "metadata": {},
   "source": [
    "展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "765112bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 'If you need a good facial scrub, you can coarsely grind some coffee beans and use them to scrub your face. They have great exfoliating properties.', 'chinese': '如果你需要优质的洁面粉，你可以粗粗磨一些咖啡豆，然后用它们打磨你的面部，能很好地去角质噢。'} \n",
      " {'input_ids': [4790, 257, 20195, 11, 19835, 2119, 11180, 2, 15525, 16977, 16977, 21639, 617, 9999, 17630, 2, 4453, 646, 896, 1444, 21639, 1301, 2119, 1163, 2, 533, 4665, 241, 605, 9981, 7708, 27164, 9, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} \n",
      " {'input_ids': [686, 37, 204, 12, 472, 58342, 55229, 2, 37, 122, 2394, 57212, 480, 52631, 239, 12220, 48648, 6, 283, 199, 8, 55229, 168, 2260, 5, 446, 53, 1325, 4908, 589, 23840, 7070, 15159, 5, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "zh_sentence = train_data[0][\"chinese\"]\n",
    "en_sentence = train_data[0][\"english\"]\n",
    "\n",
    "input = tokenizer(zh_sentence)\n",
    "target = tokenizer(text_target=en_sentence)\n",
    "print(train_data[0], \"\\n\", input, \"\\n\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa1728",
   "metadata": {},
   "source": [
    "如果没有使用 `text_target`参数则会使用源语言进行分词，产生糟糕的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ecbf25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁如果你', '需要', '优质', '的', '洁', '面', '粉', ',', '你可以', '粗', '粗', '磨', '一些', '咖啡', '豆', ',', '然后', '用', '它们', '打', '磨', '你的', '面', '部', ',', '能', '很好', '地', '去', '角', '质', '噢', '。', '</s>']\n",
      "['▁If', '▁you', '▁need', '▁a', '▁good', '▁facial', '▁scrub', ',', '▁you', '▁can', '▁co', 'arse', 'ly', '▁grind', '▁some', '▁coffee', '▁beans', '▁and', '▁use', '▁them', '▁to', '▁scrub', '▁your', '▁face', '.', '▁They', '▁have', '▁great', '▁ex', 'f', 'oli', 'ating', '▁properties', '.', '</s>']\n",
      "['▁I', 'f', '▁you', '▁need', '▁a', '▁good', '▁', 'fa', 'ci', 'al', '▁', 'sc', 'ru', 'b', ',', '▁you', '▁can', '▁', 'co', 'ar', 'se', 'ly', '▁g', 'rin', 'd', '▁some', '▁c', 'off', 'ee', '▁be', 'ans', '▁and', '▁', 'use', '▁them', '▁to', '▁', 'sc', 'ru', 'b', '▁your', '▁f', 'ace', '.', '▁They', '▁have', '▁g', 're', 'at', '▁', 'ex', 'f', 'oli', 'a', 'ting', '▁', 'pro', 'per', 'ti', 'es', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "wrong_targets = tokenizer(en_sentence)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(input[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(target[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81efefd",
   "metadata": {},
   "source": [
    "- 对于翻译任务，标签序列就是目标语言的 token ID 序列。\n",
    "- 同样需要将填充的 pad 字符设置为 -100，以便在使用交叉熵计算序列损失时将它们忽略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93e24ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([4, 45]), 'attention_mask': torch.Size([4, 45])}\n",
      "batch_y shape: torch.Size([4, 89])\n",
      "{'input_ids': tensor([[ 4790,   257, 20195,    11, 19835,  2119, 11180,     2, 15525, 16977,\n",
      "         16977, 21639,   617,  9999, 17630,     2,  4453,   646,   896,  1444,\n",
      "         21639,  1301,  2119,  1163,     2,   533,  4665,   241,   605,  9981,\n",
      "          7708, 27164,     9,     0, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000],\n",
      "        [23627,  1075,  2132,   263,  4269,  5096,  2641,   373,  6206,    15,\n",
      "          5963,   102,    69,  4086, 23421,  5251, 19835,    11,     2, 24853,\n",
      "          3493,   375, 45939,    69,  4378,   272, 21407, 22368,     9,     0,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000],\n",
      "        [  538,  8181,  1209,  7399,    75,  3890,    47,  3758,    69,  2271,\n",
      "          1209, 33967,   322,   646,  8816,  1926,  8275, 18306,  1432,  3023,\n",
      "           215, 17351,    11,  8998,     9,     0, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000, 65000,\n",
      "         65000, 65000, 65000, 65000, 65000],\n",
      "        [    7,  2620,   751,  9642,   364,  1580, 49252, 18408,    34,  1834,\n",
      "          1506,     7,    17,    16,  3219,   364,  4966, 39835,    34,  1834,\n",
      "          7940,    17, 13921,  5220,  4715,  5527,  7565,  1580, 49252,  2970,\n",
      "         21987,  3758,  7565,  3758,    16,  7565,  1580, 28314, 33105,  7565,\n",
      "          3758,    11,  2012,     9,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[   26,   589,    37,   204,    12,   472,     7,  9392,  8814,   618,\n",
      "             7, 24699,  7914,   109,     2,    37,   122,     7,  3333,  2925,\n",
      "          4009,   480,  6374, 16071,   104,   239,  2482,  5982, 10161,    32,\n",
      "         12278,     6,     7,  4694,   199,     8,     7, 24699,  7914,   109,\n",
      "           168,  3988, 18116,     5,   446,    53,  6374,   129,  2757,     7,\n",
      "          6492,   589, 23840,    86,  4867,     7,  8528,  6139,  5491,  1199,\n",
      "             5,     0,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [  218,   147,  1568,   109,    59,   239,    92,     7,   177, 11336,\n",
      "            95,   766, 10249,   177,    19,     3,   374,   104,    22,     7,\n",
      "         18043,   731,     8,   237,  2613,   373,  6206,    15,  5963,   102,\n",
      "             2,    95,   748,  3936,    95,  6213,    42,   202,   746,    14,\n",
      "             7,  4546,     7,  6492,  9747, 20406,   748,  5123,     7,  5561,\n",
      "          9250,  9868,     5,     0,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [   24,  3736,  5689,  4920,   129,  5270,  2613,  5934,   746,   618,\n",
      "           931,   672, 21845,  3335,     7,  3750,  7520,   104,  6667,  1874,\n",
      "          3289,  6046,   129,    21,    22,     7, 11816,    86,    15,  1458,\n",
      "          1296, 14327,  3988,   748,  1813,  8814,   618,  8022, 10676,    22,\n",
      "            10,     7,  8528,  4009,     5,     0,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2002,   109,  9644, 22784,  4972,   456,   450, 29552,  3708,    14,\n",
      "             3,  8374,  6206,  1199,     4,    95,   501,  5742,     7,  6231,\n",
      "           373,    15,  8776,   861, 12374,  5168,  5742,  4919, 26782,  3708,\n",
      "           800,   104,  3804,   618,     7,  6231,   373,   109, 37898,     6,\n",
      "             7,  6231,  2971,  5573,   373,   618, 12538,   748,     7,  6231,\n",
      "           373,   109, 37898,    10,  8374,  3289,   104,   804, 15282,  4509,\n",
      "            22,    27,  3100,  3253,   618,    20,  1834,  1506,    17,  2955,\n",
      "             7, 11336, 18358, 13198, 11507,  2638, 19126, 11459,     7,  6804,\n",
      "         12296,  6492,  1199,    34,  1834,  7940,    17,     5,     0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "max_input_len = 128\n",
    "max_output_len = 128\n",
    "\n",
    "inputs = [train_data[s_idx][\"chinese\"] for s_idx in range(4)]\n",
    "targets = [train_data[s_idx][\"english\"] for s_idx in range(4)]\n",
    "\n",
    "model_input = tokenizer(\n",
    "    inputs,\n",
    "    padding=True,\n",
    "    max_length=max_input_len,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "labels = tokenizer(\n",
    "    targets,\n",
    "    padding=True,\n",
    "    max_length=max_output_len,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")[\"input_ids\"]\n",
    "\n",
    "# Marian 模型会在分词结果的结尾加上特殊 token '</s>'\n",
    "end_token_index = torch.where(labels == tokenizer.eos_token_id)[1]\n",
    "\n",
    "# 特殊 token '</s>'之后进行padding\n",
    "for idx, end_idx in enumerate(end_token_index):\n",
    "    labels[idx][end_idx+1:] = -100\n",
    "\n",
    "print('batch_X shape:', {k: v.shape for k, v in model_input.items()})\n",
    "print('batch_y shape:', labels.shape)\n",
    "print(model_input)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f76d3d",
   "metadata": {},
   "source": [
    "考虑到不同模型的移位操作可能存在差异，我们通过模型自带的 prepare_decoder_input_ids_from_labels 函数来完成。完整的批处理函数为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aa6790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(65001, 512, padding_idx=65000)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=65001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "max_len = 128\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, trust_remote_code=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01beae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collote_fn(batch_samples):\n",
    "    batch_inputs, batch_targets = [], []\n",
    "    for sample in batch_samples:\n",
    "        batch_inputs.append(sample[\"chinese\"])\n",
    "        batch_targets.append(sample[\"english\"])\n",
    "    batch_data = tokenizer(\n",
    "        batch_inputs, \n",
    "        text_target=batch_targets,\n",
    "        padding=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # 在 labels 序列前添加特殊的起始 token <s>, 并去掉末尾的 token\n",
    "    batch_data[\"decoder_input_ids\"] = model.prepare_decoder_input_ids_from_labels(batch_data[\"labels\"])\n",
    "    end_token_index = torch.where(batch_data[\"labels\"] == tokenizer.eos_token_id)[1]\n",
    "    for idx, end_idx in enumerate(end_token_index):\n",
    "        batch_data[\"labels\"][idx][end_idx+1:] = -100\n",
    "    return batch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61c91454",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collote_fn)\n",
    "valid_dataloader = DataLoader(val_data, batch_size=32, shuffle=False, collate_fn=collote_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "488a9c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': tensor([[    7, 14575,  3565,  ..., 65000, 65000, 65000],\n",
      "        [ 3500, 15402, 18804,  ..., 65000, 65000, 65000],\n",
      "        [ 3279,  8803,  2064,  ..., 65000, 65000, 65000],\n",
      "        ...,\n",
      "        [    7,  6365, 27418,  ...,     9,     0, 65000],\n",
      "        [ 1824,    63,   322,  ..., 65000, 65000, 65000],\n",
      "        [    7,  1187,  1847,  ..., 65000, 65000, 65000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[19254, 58793,    15,  ...,  -100,  -100,  -100],\n",
      "        [   24, 22585,  3599,  ...,  -100,  -100,  -100],\n",
      "        [   26,    21,   419,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [ 7305,  4660,  5447,  ...,  -100,  -100,  -100],\n",
      "        [ 2900, 49171,   102,  ...,  -100,  -100,  -100],\n",
      "        [29121,   456,  3687,  ...,   611,     5,     0]]), 'decoder_input_ids': tensor([[65000, 19254, 58793,  ..., 65000, 65000, 65000],\n",
      "        [65000,    24, 22585,  ..., 65000, 65000, 65000],\n",
      "        [65000,    26,    21,  ..., 65000, 65000, 65000],\n",
      "        ...,\n",
      "        [65000,  7305,  4660,  ..., 65000, 65000, 65000],\n",
      "        [65000,  2900, 49171,  ..., 65000, 65000, 65000],\n",
      "        [65000, 29121,   456,  ...,    42,   611,     5]])})\n",
      "batch shape: {'input_ids': torch.Size([32, 36]), 'attention_mask': torch.Size([32, 36]), 'labels': torch.Size([32, 50]), 'decoder_input_ids': torch.Size([32, 50])}\n",
      "{'input_ids': tensor([[    7, 14575,  3565,  ..., 65000, 65000, 65000],\n",
      "        [ 3500, 15402, 18804,  ..., 65000, 65000, 65000],\n",
      "        [ 3279,  8803,  2064,  ..., 65000, 65000, 65000],\n",
      "        ...,\n",
      "        [    7,  6365, 27418,  ...,     9,     0, 65000],\n",
      "        [ 1824,    63,   322,  ..., 65000, 65000, 65000],\n",
      "        [    7,  1187,  1847,  ..., 65000, 65000, 65000]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[19254, 58793,    15,  ...,  -100,  -100,  -100],\n",
      "        [   24, 22585,  3599,  ...,  -100,  -100,  -100],\n",
      "        [   26,    21,   419,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [ 7305,  4660,  5447,  ...,  -100,  -100,  -100],\n",
      "        [ 2900, 49171,   102,  ...,  -100,  -100,  -100],\n",
      "        [29121,   456,  3687,  ...,   611,     5,     0]]), 'decoder_input_ids': tensor([[65000, 19254, 58793,  ..., 65000, 65000, 65000],\n",
      "        [65000,    24, 22585,  ..., 65000, 65000, 65000],\n",
      "        [65000,    26,    21,  ..., 65000, 65000, 65000],\n",
      "        ...,\n",
      "        [65000,  7305,  4660,  ..., 65000, 65000, 65000],\n",
      "        [65000,  2900, 49171,  ..., 65000, 65000, 65000],\n",
      "        [65000, 29121,   456,  ...,    42,   611,     5]])}\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(batch.keys())\n",
    "print('batch shape:', {k: v.shape for k, v in batch.items()})\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e34d708",
   "metadata": {},
   "source": [
    "## 优化模型参数\n",
    "使用 AutoModelForSeq2SeqLM 构造的模型已经封装好了对应的损失函数，并且计算出的**损失会直接包含在模型的输出 outputs 中**，可以直接通过 outputs.loss 获得，因此训练循环为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "639cafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f'loss: {0:>7f}')\n",
    "    finish_batch_num = (epoch-1) * len(dataloader)\n",
    "    \n",
    "    model.train()\n",
    "    for batch, batch_data in enumerate(dataloader, start=1):\n",
    "        batch_data = batch_data.to(device)\n",
    "        outputs = model(**batch_data)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'loss: {total_loss/(finish_batch_num + batch):>7f}')\n",
    "        progress_bar.update(1)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65816609",
   "metadata": {},
   "source": [
    "验证/测试循环负责评估模型的性能。对于翻译任务，经典的评估指标是 Kishore Papineni 等人在[《BLEU: a Method for Automatic Evaluation of Machine Translation》](https://aclanthology.org/P02-1040.pdf)中提出的 [BLEU ](https://en.wikipedia.org/wiki/BLEU)值，用于度量两个词语序列之间的一致性，但是其并不会衡量语义连贯性或者语法正确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32da02e3",
   "metadata": {},
   "source": [
    "由于计算 BLEU 值需要输入分好词的文本，而不同的分词方式会对结果造成影响，因此现在更常用的评估指标是 [SacreBLEU](https://github.com/mjpost/sacrebleu)，它对分词的过程进行了标准化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66833427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.750469682990165\n",
      "1.683602693167689\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "bad_predictions_1 = [\"This This This This\"]\n",
    "bad_predictions_2 = [\"This plugin\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "bleu = BLEU()\n",
    "print(bleu.corpus_score(predictions, references).score)\n",
    "print(bleu.corpus_score(bad_predictions_1, references).score)\n",
    "print(bleu.corpus_score(bad_predictions_2, references).score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6dd539",
   "metadata": {},
   "source": [
    "SacreBLEU 默认会采用 mteval-v13a.pl 分词器对文本进行分词，但是它无法处理中文、日文等非拉丁系语言。**对于中文就需要设置参数 tokenize='zh' 手动使用中文分词器**，否则会计算出不正确的 BLEU 值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a55adba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 45.340106118883256\n",
      "wrong BLEU: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "predictions = [\n",
    "    \"我在苏州大学学习计算机，苏州大学很美丽。\"\n",
    "]\n",
    "\n",
    "references = [\n",
    "    [\n",
    "        \"我在环境优美的苏州大学学习计算机。\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "bleu = BLEU(tokenize='zh')\n",
    "print(f'BLEU: {bleu.corpus_score(predictions, references).score}')\n",
    "bleu = BLEU()\n",
    "print(f'wrong BLEU: {bleu.corpus_score(predictions, references).score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d53c62",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "model = model.to(device)\n",
    "\n",
    "sentence = '我叫张三，我住在苏州。'\n",
    "\n",
    "sentence_inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "sentence_generated_tokens = model.generate(\n",
    "    sentence_inputs[\"input_ids\"],\n",
    "    attention_mask=sentence_inputs[\"attention_mask\"],\n",
    "    max_length=128\n",
    ")\n",
    "sentence_decoded_pred = tokenizer.decode(sentence_generated_tokens[0], skip_special_tokens=True)\n",
    "print(sentence_decoded_pred)```\n",
    "\n",
    "- outputs:\n",
    "```\n",
    "Using cpu device  \n",
    "My name is Zhang San, and I live in Suzhou.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e45d8",
   "metadata": {},
   "source": [
    "在 `generate()` 生成 token ID 之后，我们通过分词器自带的 `tokenizer.batch_decode()` 函数将 batch 中所有的 token ID 序列都转换为文本，因此翻译多个句子也没有问题：\n",
    "```python\n",
    "sentences = ['我叫张三，我住在苏州。', '我在环境优美的苏州大学学习计算机。']\n",
    "\n",
    "sentences_inputs = tokenizer(\n",
    "    sentences, \n",
    "    padding=True, \n",
    "    max_length=128,\n",
    "    truncation=True, \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "sentences_generated_tokens = model.generate(\n",
    "    sentences_inputs[\"input_ids\"],\n",
    "    attention_mask=sentences_inputs[\"attention_mask\"],\n",
    "    max_length=128\n",
    ")\n",
    "sentences_decoded_preds = tokenizer.batch_decode(sentences_generated_tokens, skip_special_tokens=True)\n",
    "print(sentences_decoded_preds)\n",
    "```\n",
    "```\n",
    "[\n",
    "    'My name is Zhang San, and I live in Suzhou.', \n",
    "    \"I'm studying computers at Suzhou University in a beautiful environment.\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b14f3b",
   "metadata": {},
   "source": [
    "- “验证/测试循环”\n",
    "    - model.generate() 函数获取预测结果\n",
    "    - 结果和正确标签都处理为 SacreBLEU 接受的文本列表形式\n",
    "    - 最后送入到 SacreBLEU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22d22f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.metrics import BLEU\n",
    "import numpy as np\n",
    "bleu = BLEU()\n",
    "\n",
    "def test_loop(dataloader, model):\n",
    "    preds, labels =[], []\n",
    "\n",
    "    model.eval()\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        with torch.no_grad():\n",
    "            generated_tokens =model.generate(\n",
    "                batch_data[\"input_ids\"],\n",
    "                attention_mask=batch_data[\"attention_mask\"],\n",
    "                max_length = max_len,\n",
    "            ).cpu().numpy()\n",
    "            label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "            \n",
    "            decode_preds = tokenizer.batch_decode(generated_tokens, \n",
    "                                                  skip_special_tokens=True)\n",
    "            label_tokens = np.where(label_tokens != -100,    # 条件：检查每个元素是否不等于-100\n",
    "                                    label_tokens,            # 如果True：保持原值不变\n",
    "                                    tokenizer.pad_token_id)  # 如果False：替换为pad_token_id\n",
    "            decode_labels = tokenizer.batch_decode(label_tokens, \n",
    "                                                   skip_special_tokens=True)\n",
    "            \n",
    "            preds += [pred.strip() for pred in decode_preds] \n",
    "            labels += [[label.strip()] for label in decode_labels]\n",
    "    return bleu.corpus_score(preds, labels).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "821cae92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c4ac1f",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648eb142",
   "metadata": {},
   "source": [
    "在训练之前，我们先评估一下没有微调的模型在测试集上的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2e72614",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1229/1229 [07:06<00:00,  2.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42.610827239170156"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "test_loop(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ead6dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.573716: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6250/6250 [07:40<00:00, 13.57it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [03:29<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 46.35\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Epoch 2/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.501357: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6250/6250 [07:42<00:00, 13.52it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 625/625 [03:30<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 57.07\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Epoch 3/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 2.457017: 100%|██████████████████████████████████████████████████████| 6250/6250 [07:44<00:00, 13.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [03:27<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 49.90\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "learning_rate = 1e-5\n",
    "epoch_num = 3   \n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=epoch_num*len(train_dataloader),\n",
    ")\n",
    "\n",
    "total_loss = 0.\n",
    "best_bleu = 0.\n",
    "for t in range(epoch_num):\n",
    "    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    valid_bleu = test_loop(valid_dataloader, model)\n",
    "    print(f\"BLEU: {valid_bleu:>0.2f}\\n\")\n",
    "    if valid_bleu > best_bleu:\n",
    "        best_bleu = valid_bleu\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(model.state_dict(), f'epoch_{t+1}_valid_bleu_{valid_bleu:0.2f}_model_weights.bin')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53705f",
   "metadata": {},
   "source": [
    "## 测试模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aac4b48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1229/1229 [07:00<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BLEU: 54.87\n",
      "\n",
      "saving predicted results...\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collote_fn)\n",
    "\n",
    "import json\n",
    "\n",
    "model.load_state_dict(torch.load(\"epoch_2_valid_bleu_57.07_model_weights.bin\"))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print('evaluating on test set...')\n",
    "    sources, preds, labels = [], [], []\n",
    "    for batch_data in tqdm(test_dataloader):\n",
    "        batch_data = batch_data.to(device)\n",
    "        generated_tokens = model.generate(\n",
    "            batch_data[\"input_ids\"],\n",
    "            attention_mask=batch_data[\"attention_mask\"],\n",
    "            max_length=max_len,\n",
    "        ).cpu().numpy()\n",
    "        label_tokens = batch_data[\"labels\"].cpu().numpy()\n",
    "\n",
    "        decoded_sources = tokenizer.batch_decode(\n",
    "            batch_data[\"input_ids\"].cpu().numpy(), \n",
    "            skip_special_tokens=True, \n",
    "            use_source_tokenizer=True\n",
    "        )\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        label_tokens = np.where(label_tokens != -100, label_tokens, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(label_tokens, skip_special_tokens=True)\n",
    "\n",
    "        sources += [source.strip() for source in decoded_sources]\n",
    "        preds += [pred.strip() for pred in decoded_preds]\n",
    "        labels += [[label.strip()] for label in decoded_labels]\n",
    "    bleu_score = bleu.corpus_score(preds, labels).score\n",
    "    print(f\"Test BLEU: {bleu_score:>0.2f}\\n\")\n",
    "    results = []\n",
    "    print('saving predicted results...')\n",
    "    for source, pred, label in zip(sources, preds, labels):\n",
    "        results.append({\n",
    "            \"sentence\": source, \n",
    "            \"prediction\": pred, \n",
    "            \"translation\": label[0]\n",
    "        })\n",
    "    with open('test_data_pred.json', 'wt', encoding='utf-8') as f:\n",
    "        for exapmle_result in results:\n",
    "            f.write(json.dumps(exapmle_result, ensure_ascii=False) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
