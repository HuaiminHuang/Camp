{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "091b4ac8",
   "metadata": {},
   "source": [
    "# 搭建 Transformer\n",
    "- encoder + decoder\n",
    "![](./img/TransformerBlock.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "651dd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d3e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMHA(nn.Module):\n",
    "    def __init__(self, d_model, h):\n",
    "        \"\"\"\n",
    "        多头注意力机制\n",
    "        args:\n",
    "            d_model:输入序列的维度大小\n",
    "            h：attention heads的数量 \n",
    "        \"\"\"\n",
    "        super(TransformerMHA, self).__init__()\n",
    "        assert d_model % h ==0, \"d_model must be divided by h (number of heads)\"\n",
    "\n",
    "        self.d_model = d_model \n",
    "        self.h = h\n",
    "\n",
    "        # QKV Linear layer\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        args: \n",
    "            i = q,k,v shape is (B, seq_len_{i}, d_model)\n",
    "            mask shape is (B, 1, seq_len_q, seq_len_k)\n",
    "            and seq_len_k = seq_len_v unequal to seq_len_q\n",
    "        return:\n",
    "            outputs:(weighted) shape is (batch_size, h, seq_len_q, seq_len_kv)\n",
    "            attntion_weights shape is (batch_size, h, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len_q, _ = q.size()\n",
    "        seq_len_kv, _ , _ = k.size()\n",
    "\n",
    "        # 将d_model 拆分为 head_numbers and head_dim \n",
    "        # 并且 (B, S_ , num_head, head_dim)  ---> (B , num_head, , S_, head_dim)\n",
    "        Q = self.w_q(q).view(batch_size, seq_len_q, self.h, -1).transpose(1,2)\n",
    "        K = self.w_k(k).view(batch_size, seq_len_kv, self.h, -1).transpose(1,2)\n",
    "        V = self.w_v(v).view(batch_size, seq_len_kv, self.h, -1).transpose(1,2)\n",
    "\n",
    "        scaled_attention, _ = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
    "\n",
    "        concat_out = scaled_attention.transpose(1,2).contiguous()\n",
    "        concat_out.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        out = self.fc_out(concat_out)\n",
    "        return out \n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    缩放点积注意力计算。\n",
    "\n",
    "    args:\n",
    "        Q: 查询矩阵 (batch_size, num_heads, seq_len_q, d_k)\n",
    "        K: 键矩阵 (batch_size, num_heads, seq_len_kv, d_k)\n",
    "        V: 值矩阵 (batch_size, num_heads, seq_len_kv, d_v)\n",
    "        mask: 掩码矩阵 (batch_size, 1, seq_len_q, seq_len_kv) 或 \n",
    "        (1, 1, seq_len_q, seq_len_kv) 或 \n",
    "        (batch_size, h, seq_len_q, seq_len_kv)\n",
    "\n",
    "    return:\n",
    "        output: 注意力加权后的输出矩阵\n",
    "        attention_weights: 注意力权重矩阵\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)  # d_k\n",
    "\n",
    "    # 计算点积并进行缩放  \n",
    "    # !! K.transpose(-2, -1)这里如果使用正的索引会造成维度错误，\n",
    "    # 例如使用(1, 2)如果新增h-head_number(B, h, S, h_dim).\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # 如果提供了掩码矩阵，则将掩码对应位置的分数设为 -inf\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # 对缩放后的分数应用 Softmax 函数，得到注意力权重\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # 加权求和，计算输出\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "\n",
    "    return output, attention_weights       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6c4f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        FFN\n",
    "        args:\n",
    "            d_model: 输入和输出向量的维度\n",
    "            d_ff： FFN隐藏层的维度\n",
    "            dropout：随机屏蔽部分输出，防止过拟合（也是一种正则化手段）\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward,self).__init__()\n",
    "        self.proj_up = nn.Linear(d_model, d_ff)\n",
    "        self.proj_down = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj_up(x).relu()\n",
    "        x = self.dropout(x)\n",
    "        x = self.proj_down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "799dd0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((512, 512), eps=1e-09, elementwise_affine=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LN = nn.LayerNorm((512,512), eps=1e-9)\n",
    "LN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe30877",
   "metadata": {},
   "source": [
    "## encoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba4ae19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, h, dropout=0.1):\n",
    "        \"\"\"\n",
    "        encoder\n",
    "\n",
    "        args:\n",
    "            d_model: 嵌入维度\n",
    "            h: head numbers\n",
    "            d_ff: FFN hidden dimantion\n",
    "            dropout: Dropout probs\n",
    "        \"\"\"\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.mha = TransformerMHA(d_model, h)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        \"\"\"\n",
    "        x shape is (batch_size, sqe_len, d_model)\n",
    "        \n",
    "        args:\n",
    "            x: inputs\n",
    "            src_mask: self-attention mask\n",
    "        return:\n",
    "            x: encoderlayer output shape is (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.dropout(self.mha(x, src_mask)) +x\n",
    "        x = self.ln1(x)\n",
    "        x = self.dropout(self.ffn(x)) + x\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46c802bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderLayer(\n",
       "  (mha): TransformerMHA(\n",
       "    (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (ffn): PositionwiseFeedForward(\n",
       "    (proj_up): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (proj_down): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = EncoderLayer(512, 2048, 8)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970dee1d",
   "metadata": {},
   "source": [
    "另一种模块化的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db96631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "        子层连接的另一种实现方式，残差连接直接在该模块中实现。\n",
    "\n",
    "        参数:\n",
    "            feature_size: 输入特征的维度大小，即归一化的特征维度。\n",
    "            dropout: 残差连接中的 Dropout 概率。\n",
    "            epsilon: 防止除零的小常数。\n",
    "        \"\"\"\n",
    "    def __init__(self, feature_size, dropout=0.1, epsilon=1e-9):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = nn.LayerNorm(feature_size, eps=epsilon)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # 将子层输出应用 dropout 后经过残差连接后再进行归一化，可见本文「呈现」部分\n",
    "        return self.norm(x + self.dropout(sublayer(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8ff85aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer2(nn.Module):\n",
    "    def __init__(self, d_model, h, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        编码器层。\n",
    "        \n",
    "        参数:\n",
    "            d_model: 嵌入维度\n",
    "            h: 多头注意力的头数\n",
    "            d_ff: 前馈神经网络的隐藏层维度\n",
    "            dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        super(EncoderLayer2, self).__init__()\n",
    "        self.self_attn = TransformerMHA(d_model, h)  # 多头自注意力（Multi-Head Self-Attention）\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)  # 前馈神经网络\n",
    "        \n",
    "        # 定义两个子层连接，分别用于多头自注意力和前馈神经网络（对应模型架构图中的两个残差连接）\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "            x: 输入张量，形状为 (batch_size, seq_len, d_model)。\n",
    "            src_mask: 源序列掩码，用于自注意力。\n",
    "\n",
    "        返回:\n",
    "            编码器层的输出，形状为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, src_mask))  # 自注意力子层\n",
    "        x = self.sublayers[1](x, self.feed_forward)  # 前馈子层\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4bfa91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderLayer2(\n",
       "  (self_attn): TransformerMHA(\n",
       "    (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (feed_forward): PositionwiseFeedForward(\n",
       "    (proj_up): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (proj_down): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sublayers): ModuleList(\n",
       "    (0-1): 2 x SublayerConnection(\n",
       "      (norm): LayerNorm((512,), eps=1e-09, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = EncoderLayer2(512, 8, 2048, 0.1)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7432afdc",
   "metadata": {},
   "source": [
    "# Decoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6274787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, h, dropout):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        \"\"\"\n",
    "        decoder\n",
    "\n",
    "        args:\n",
    "            d_model: 嵌入维度\n",
    "            h: 多头注意力的头数\n",
    "            d_ff: 前馈神经网络的隐藏层维度\n",
    "            dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        self.self_mha = TransformerMHA(d_model, h)\n",
    "        self.corss_attention = TransformerMHA(d_model, h)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ln3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, menmory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            x: 解码器输入 (batch_size, seq_len_tgt, d_model)\n",
    "            memory: 编码器输出 (batch_size, seq_len_src, d_model)\n",
    "            src_mask: 源序列掩码，用于交叉注意力\n",
    "            tgt_mask: 目标序列掩码，用于自注意力\n",
    "        return:\n",
    "            x decoder outputs\n",
    "        \"\"\"\n",
    "        x = self.ln1(self.dropout(self.self_mha(x, x, x, tgt_mask)) + x)\n",
    "        x = self.ln2(self.dropout(\n",
    "            self.corss_attention(x, menmory, menmory, src_mask)\n",
    "        )+ x)\n",
    "        x = self.ln3(self.dropout(self.ffn(x))) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1877852f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderLayer(\n",
       "  (self_mha): TransformerMHA(\n",
       "    (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (corss_attention): TransformerMHA(\n",
       "    (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (ffn): PositionwiseFeedForward(\n",
       "    (proj_up): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (proj_down): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (ln3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = DecoderLayer(512, 2048, 8, 0.1)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded32c8c",
   "metadata": {},
   "source": [
    "另一种实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54e4ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer2(nn.Module):\n",
    "    def __init__(self, d_model, h, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        解码器层。\n",
    "        \n",
    "        参数:\n",
    "            d_model: 嵌入维度\n",
    "            h: 多头注意力的头数\n",
    "            d_ff: 前馈神经网络的隐藏层维度\n",
    "            dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        super(DecoderLayer2, self).__init__()\n",
    "        self.self_attn = TransformerMHA(d_model, h)  # 掩码多头自注意力（Masked Multi-Head Self-Attention）\n",
    "        self.cross_attn = TransformerMHA(d_model, h)  # 多头交叉注意力（Multi-Head Cross-Attention）\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)  # 前馈神经网络\n",
    "        \n",
    "        # 定义三个子层连接，分别用于掩码多头自注意力、多头交叉注意力和前馈神经网络（对应模型架构图中的三个残差连接）\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(3)])\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        参数:\n",
    "            x: 解码器输入 (batch_size, seq_len_tgt, d_model)\n",
    "            memory: 编码器输出 (batch_size, seq_len_src, d_model)\n",
    "            src_mask: 源序列掩码，用于交叉注意力\n",
    "            tgt_mask: 目标序列掩码，用于自注意力\n",
    "        返回:\n",
    "            x: 解码器层的输出\n",
    "        \"\"\"\n",
    "        # 第一个子层：掩码多头自注意力（Masked Multi-Head Self-Attention）\n",
    "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        \n",
    "        # 第二个子层：交叉多头注意力（Multi-Head Cross-Attention），使用编码器的输出 memory\n",
    "        x = self.sublayers[1](x, lambda x: self.cross_attn(x, memory, memory, src_mask))\n",
    "        \n",
    "        # 第三个子层：前馈神经网络\n",
    "        x = self.sublayers[2](x, self.feed_forward)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd77d852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderLayer2(\n",
       "  (self_attn): TransformerMHA(\n",
       "    (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (cross_attn): TransformerMHA(\n",
       "    (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (feed_forward): PositionwiseFeedForward(\n",
       "    (proj_up): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (proj_down): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sublayers): ModuleList(\n",
       "    (0-2): 3 x SublayerConnection(\n",
       "      (norm): LayerNorm((512,), eps=1e-09, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = DecoderLayer2(512, 8, 2048, 0.1)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a318d",
   "metadata": {},
   "source": [
    "# 编码器和解码器 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03f60d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, h, N, dropout=0.1):\n",
    "        \"\"\"\n",
    "        编码器由 N 个EncoderLayer 构成\n",
    "\n",
    "        args:\n",
    "            d_model: 嵌入维度\n",
    "            N: 编码器层的数量\n",
    "            h: 多头注意力的头数\n",
    "            d_ff: 前馈神经网络的隐藏层维度\n",
    "            dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, d_ff, h, dropout) for _ in range(N)]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x) # 最后层归一化\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14173b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (mha): TransformerMHA(\n",
       "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ffn): PositionwiseFeedForward(\n",
       "        (proj_up): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (proj_down): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EncoderBlock = Encoder(512, 2048, 8, 6, 0.1)\n",
    "EncoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6718ad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, h, N, dropout=0.1):\n",
    "        \"\"\"\n",
    "        解码器，由 N 个 DecoderLayer 堆叠而成。\n",
    "        \n",
    "        args:\n",
    "            d_model: 嵌入维度\n",
    "            N: 解码器层的数量\n",
    "            h: 多头注意力的头数\n",
    "            d_ff: 前馈神经网络的隐藏层维度\n",
    "            dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, d_ff, h, dropout) for _ in range(N)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)  # 最后层归一化\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        args:\n",
    "            x: 解码器输入 (batch_size, seq_len_tgt, d_model)\n",
    "            memory: 编码器的输出 (batch_size, seq_len_src, d_model)\n",
    "            src_mask: 用于交叉注意力的源序列掩码\n",
    "            tgt_mask: 用于自注意力的目标序列掩码\n",
    "        return:\n",
    "            decoder outputs\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)  # 最后层归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed691e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_mha): TransformerMHA(\n",
       "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (corss_attention): TransformerMHA(\n",
       "        (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (ffn): PositionwiseFeedForward(\n",
       "        (proj_up): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (proj_down): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecoderBlock = Decoder(512, 2048, 8, 6, 0.1)\n",
    "DecoderBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aecaa40",
   "metadata": {},
   "source": [
    "其他组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07252640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq, pad_token_id=0):\n",
    "    # seq shape is [batch_size, seq_len] --> [batch_size, 1, 1, seq_len]\n",
    "    mask = (seq != pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "    return mask\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.tril(torch.ones(size, size)).type(torch.bool)  # 下三角矩阵\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa18f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    token ID tranform to embedding vector\n",
    "\n",
    "    args: \n",
    "        vocab_size: 词表大小\n",
    "        d_model：嵌入向量维度（隐藏层维度）\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.scaled_factor = math.sqrt(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x_embed = x * self.scaled_factor\n",
    "        return x_embed\n",
    "\n",
    "class PositionEmbeding(nn.Module):\n",
    "    def __init__(self,d_model, dropout =0.1, max_len=5000):\n",
    "        \"\"\"\n",
    "        PE,添加序列的唯一位置信息\n",
    "\n",
    "        args:\n",
    "            d_model: 嵌入维度\n",
    "            dropout: 应用PE后的 Dropout的概率\n",
    "            max_len: 位置编码的最大长，适应不同的输入序列\n",
    "        \"\"\"\n",
    "        super(PositionEmbeding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)  # 论文的5.4 Residual Dropout\n",
    "        \n",
    "        # 创建位置编码矩阵 shape is (max_len, d_model)\n",
    "        # 位置索引为 (max_len, 1)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1) # unsqueeze(1) to broadcast d_model\n",
    "\n",
    "        # 频率计算\n",
    "        # pos / (10000^(2i/d_model)) \n",
    "        # = pos * exp(log(10000^(-2i/d_model)))\n",
    "        # = pos * exp(-2i/d_model * log(10000))\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model )\n",
    "        )\n",
    "\n",
    "        # 计算 sin 和 cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "\n",
    "        # 扩充维度广播 input (batch_size, seq_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # 将位置编码注册为模型的缓冲区，不作为参数更新\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]  # 去除相同序列长度的位置编码进行合并\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class SourceEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, src_vocab_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        inputs 部分的 embeddings\n",
    "\n",
    "        args:\n",
    "            scr_vocab_size: 源语言词汇表大小\n",
    "            d_model: 嵌入向量维度\n",
    "            dropout概率 \n",
    "        \"\"\"\n",
    "        super(SourceEmbedding, self).__init__()\n",
    "        self.embed = Embeddings(src_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionEmbeding(d_model, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(self.embed(x))  # (batch_size, seq_len_src, d_model)\n",
    "        return x\n",
    "    \n",
    "class TargetEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, tgt_vocab_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        inputs 部分的 embeddings\n",
    "\n",
    "        args:\n",
    "            tgt_vocab_size: 源语言词汇表大小\n",
    "            d_model: 嵌入向量维度\n",
    "            dropout概率 \n",
    "        \"\"\"\n",
    "        super(TargetEmbedding, self).__init__()\n",
    "        self.embed = Embeddings(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionEmbeding(d_model, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(self.embed(x))  # (batch_size, seq_len_tgt, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f300578",
   "metadata": {},
   "source": [
    "# 最后一步构建完整的 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d0752d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, N, h, d_ff, dropout =0.1):\n",
    "        \"\"\"\n",
    "        Tansformer Architecture\n",
    "\n",
    "        args:\n",
    "            src_vocab_size: 源语言词汇表大小\n",
    "            tgt_vocab_size: 目标语言词汇表大小\n",
    "            d_model: 嵌入维度\n",
    "            N: 编码器和解码器的层数\n",
    "            h: 多头注意力的头数\n",
    "            d_ff: 前馈神经网络的隐藏层维度\n",
    "            dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        super(Transformer,self).__init__()\n",
    "\n",
    "        # inputs and outputs cross embedinglayer & add position embeding\n",
    "        self.src_embed = SourceEmbedding(d_model, src_vocab_size, dropout)\n",
    "        self.tgt_embed = TargetEmbedding(d_model, tgt_vocab_size, dropout)\n",
    "\n",
    "        # 编码器和解码器\n",
    "        self.encoder = Encoder(d_model, d_ff, h, N, dropout)\n",
    "        self.decoder = Decoder(d_model, d_ff, h, N, dropout)\n",
    "\n",
    "        # 输出线性层\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            src: 源序列输入 (batch_size, seq_len_src)\n",
    "            tgt: 目标序列输入 (batch_size, seq_len_tgt)\n",
    "\n",
    "        return:\n",
    "            Transformer 的输出（未经过 Softmax）\n",
    "        \"\"\"\n",
    "        # 生成掩码，\n",
    "        src_mask = create_padding_mask(src)\n",
    "        tgt_mask = create_look_ahead_mask(tgt)\n",
    "\n",
    "        # 编码\n",
    "        encoder_out = self.encoder(self.src_embed(src), src_mask)\n",
    "        decoder_out = self.decoder(self.tgt_embed(tgt), encoder_out, src_mask, tgt_mask)\n",
    "\n",
    "        out = self.fc_out(decoder_out)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92c2974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (src_embed): SourceEmbedding(\n",
      "    (embed): Embeddings(\n",
      "      (embed): Embedding(5000, 512)\n",
      "    )\n",
      "    (positional_encoding): PositionEmbeding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (tgt_embed): TargetEmbedding(\n",
      "    (embed): Embeddings(\n",
      "      (embed): Embedding(5000, 512)\n",
      "    )\n",
      "    (positional_encoding): PositionEmbeding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x EncoderLayer(\n",
      "        (mha): TransformerMHA(\n",
      "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (proj_up): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (proj_down): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x DecoderLayer(\n",
      "        (self_mha): TransformerMHA(\n",
      "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (corss_attention): TransformerMHA(\n",
      "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (proj_up): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (proj_down): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (fc_out): Linear(in_features=512, out_features=5000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 定义词汇表大小（根据数据集）\n",
    "src_vocab_size = 5000  # 源语言词汇表大小\n",
    "tgt_vocab_size = 5000  # 目标语言词汇表大小\n",
    "\n",
    "# 使用 Transformer base 参数\n",
    "d_model = 512      # 嵌入维度\n",
    "N = 6              # 编码器和解码器的层数\n",
    "h = 8              # 多头注意力的头数\n",
    "d_ff = 2048        # 前馈神经网络的隐藏层维度\n",
    "dropout = 0.1      # Dropout 概率\n",
    "\n",
    "# 实例化模型\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=d_model,\n",
    "    N=N,\n",
    "    h=h,\n",
    "    d_ff=d_ff,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# 打印模型架构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b21607f",
   "metadata": {},
   "source": [
    "**对照pytorch里面的transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad9b1380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhm18\\miniconda3\\envs\\TrainingCamp\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "transformer_model = nn.Transformer(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048\n",
    ")\n",
    "\n",
    "# 直接打印模型结构\n",
    "print(transformer_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
