{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacab750",
   "metadata": {},
   "source": [
    "# 微调预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b6c2ab",
   "metadata": {},
   "source": [
    "- 任务：微调一个句子对分类模型（保存验证集上的最好模型权重）\n",
    "- Dataset：模型。我们选择蚂蚁金融语义相似度数据集 AFQMC 作为语料\n",
    "    - `{\"sentence1\": \"还款还清了，为什么花呗账单显示还要还款\", \"sentence2\": \"花呗全额还清怎么显示没有还款\", \"label\": \"1\"}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcb207c",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "### **首先继承 Dataset 类构造自定义数据集，以组织样本和标签。**\n",
    "- 如果数据集非常巨大，难以一次性加载到内存中，我们也可以继承 IterableDataset 类构建迭代型数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8c1646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class AFQMC(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        self.data = self.load_data(data_file)\n",
    "\n",
    "    def load_data(self, data_file, ):\n",
    "        Data = {}\n",
    "        with open (data_file, \"rt\", encoding=\"utf-8\") as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                sample = json.loads(line.strip())\n",
    "                Data[idx] = sample\n",
    "        return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665454d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hhm18\\\\Desktop\\\\course'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9f80128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'sentence1': '蚂蚁借呗等额还款可以换成先息后本吗', 'sentence2': '借呗有先息到期还本吗', 'label': '0'},\n",
       " 34334)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = AFQMC(\"dataset/AFQMC/train.json\")\n",
    "valid_data = AFQMC(\"dataset/AFQMC/dev.json\")\n",
    "train_data[0], len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137d210d",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "按照批次进行数据加载，并且转化为需要的格式。(padding, trancated...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97ca75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhm18\\miniconda3\\envs\\TrainingCamp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd9172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-base-chinese\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2006efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collote_fn(batch_samples):\n",
    "    batch_samples_1, batch_samples_2 =[], []\n",
    "    batch_label = []\n",
    "    for sample in batch_samples:\n",
    "        batch_samples_1.append(sample[\"sentence1\"])\n",
    "        batch_samples_2.append(sample[\"sentence2\"])\n",
    "        batch_label.append(int(sample['label']))\n",
    "        \n",
    "    # 转换为张量\n",
    "    X = tokenizer(\n",
    "        batch_samples_1, batch_samples_2, \n",
    "        padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    y = torch.tensor(batch_label)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204da12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_data, batch_size=16, shuffle=True, collate_fn=collote_fn\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_data, batch_size=16, shuffle=True, collate_fn=collote_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e09b5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2146, 270)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader), len(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "006167d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_X shape: {'input_ids': torch.Size([16, 65]), 'token_type_ids': torch.Size([16, 65]), 'attention_mask': torch.Size([16, 65])}\n",
      "batch_y shape: torch.Size([16])\n",
      "{'input_ids': tensor([[ 101,  711,  784,  ...,    0,    0,    0],\n",
      "        [ 101, 2769,  679,  ...,    0,    0,    0],\n",
      "        [ 101, 5709, 1446,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 1377,  809,  ...,    0,    0,    0],\n",
      "        [ 101,  711,  784,  ...,    0,    0,    0],\n",
      "        [ 101, 5709, 1446,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "print('batch_X shape:', {k: v.shape for k, v in batch_X.items()})\n",
    "print('batch_y shape:', batch_y.shape)\n",
    "print(batch_X)\n",
    "print(batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e145e9bd",
   "metadata": {},
   "source": [
    "可以看到，DataLoader 按照我们设置的 batch size 每次对 4 个样本进行编码，并且通过设置 padding=True 和 truncation=True 来自动对每个 batch 中的样本进行补全和截断。\n",
    "- [CLS]--101 [SPE]--102\n",
    "- 按照BERT将样本处理成了[CLS] ...[SEP] ... [SEP]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7046e",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "可以直接使用HF的`AutoModelForSentenceClassification`，但是实际中需要一些自定义操作。\n",
    "最简单的方法是加载BERT，然后加一个全连接层完成分类任务"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5ae4d",
   "metadata": {},
   "source": [
    "```python\n",
    "from torch import nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class BertForPairewiseCLS(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertForPairewiseCLS, self).__init__()\n",
    "        self.bert_encoder = AutoModel.from_pretrained(checkpoint)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, 2) # 这里的label只有两个1，0\n",
    "\n",
    "    def forward(self, x):\n",
    "        bert_output = self.bert_encoder(**x)\n",
    "        cls_vecter = bert_output.last_hidden_state[:, 0, :] # 手动提取CLS\n",
    "        cls_vecter = self.dropout(cls_vecter)  # 手动添加dropout\n",
    "        logits = self.classifier(cls_vecter)   # 手动分类\n",
    "        return logits\n",
    "\n",
    "model = BertForPairewiseCLS().to(device)\n",
    "print(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f8281",
   "metadata": {},
   "source": [
    "更为常见的写法是继承 Transformers 库中的预训练模型来创建自己的模型。例如这里我们可以继承 BERT 模型（BertPreTrainedModel 类）来创建一个与上面模型结构完全相同的分类器：\n",
    "```python\n",
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "class BertForPairwiseCLS(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(768, 2)\n",
    "        self.post_init()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bert_output = self.bert(**x)\n",
    "        cls_vectors = bert_output.last_hidden_state[:, 0, :]\n",
    "        cls_vectors = self.dropout(cls_vectors)\n",
    "        logits = self.classifier(cls_vectors)\n",
    "        return logits\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = BertForPairwiseCLS.from_pretrained(checkpoint, config=config).to(device)\n",
    "print(model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e13d4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPairwiseCLS were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForPairwiseCLS(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from transformers import AutoConfig\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device} device')\n",
    "\n",
    "class BertForPairwiseCLS(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(768, 2)\n",
    "        self.post_init()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        bert_output = self.bert(**x)\n",
    "        cls_vectors = bert_output.last_hidden_state[:, 0, :]\n",
    "        cls_vectors = self.dropout(cls_vectors)\n",
    "        logits = self.classifier(cls_vectors)\n",
    "        return logits\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "model = BertForPairwiseCLS.from_pretrained(checkpoint, config=config).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a468fe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "# 测试输出的shape\n",
    "outputs = model(batch_X.to(device))\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781aaef5",
   "metadata": {},
   "source": [
    "## 优化模型的参数\n",
    "每一轮 Epoch 分为训练循环和验证/测试循环。在训练循环中计算损失、优化模型的参数，在验证/测试循环中评估模型的性能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcab4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, \n",
    "               lr_shedular, epoch, total_loss):\n",
    "    progress_bar = tqdm(range(len(dataloader)))\n",
    "    progress_bar.set_description(f\"loss: {0:>7f}\")\n",
    "    finish_step_num = (epoch-1) * len(dataloader)\n",
    "\n",
    "    model.train()\n",
    "    for step, (X, y) in enumerate(dataloader, start=1):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_shedular.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f\"loss{total_loss/(finish_step_num + step):>7f}\")\n",
    "        progress_bar.update(1)\n",
    "    return total_loss\n",
    "\n",
    "def test_loop(dataloader, model, mode=\"Test\"):\n",
    "    assert mode in [\"Valid\", \"Test\"] \n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            # 对于[batch_size, num_classes]取出标签维度的最大索引得到预测标签，\n",
    "            # 和真实标签进行比较 返回bool，在进行01转换求和出正确的总数\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    correct /=size\n",
    "    print(f\"{mode}Accuracy:{(100 * correct):>0.1f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab49271",
   "metadata": {},
   "source": [
    "Transformers 库同样实现了很多的优化器，并且相比 Pytorch 固定学习率，Transformers 库的优化器会随着训练过程逐步减小学习率（通常会产生更好的效果）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ba1e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss0.610267: 100%|████████████████████████████████████████████████████████████████| 2146/2146 [04:36<00:00,  7.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValidAccuracy:69.0%\n",
      "\n",
      "Epoch 2/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss0.587256: 100%|████████████████████████████████████████████████████████████████| 2146/2146 [04:35<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValidAccuracy:69.3%\n",
      "\n",
      "Epoch 3/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss0.556291: 100%|████████████████████████████████████████████████████████████████| 2146/2146 [04:31<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValidAccuracy:68.4%\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "epoch =3\n",
    "lr = 1e-5\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "num_training_steps = epoch * len(train_dataloader)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "total_loss = 0.\n",
    "for t in range(epoch):\n",
    "    print(f\"Epoch {t+1}/{epoch}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, loss_fn, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    test_loop(valid_dataloader, model, mode='Valid')\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46d379f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss0.442641: 100%|████████████████████████████████████████████████████████████████| 2146/2146 [04:30<00:00,  7.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValidAccuracy:68.4%\n",
      "\n",
      "saving new weights...\n",
      "\n",
      "Epoch 2/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss0.443197: 100%|████████████████████████████████████████████████████████████████| 2146/2146 [04:30<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValidAccuracy:68.4%\n",
      "\n",
      "Epoch 3/3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss0.443389: 100%|████████████████████████████████████████████████████████████████| 2146/2146 [04:34<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValidAccuracy:68.4%\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def test_loop(dataloader, model, mode=\"Test\"):\n",
    "    assert mode in [\"Valid\", \"Test\"] \n",
    "    size = len(dataloader.dataset)\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            # 对于[batch_size, num_classes]取出标签维度的最大索引得到预测标签，\n",
    "            # 和真实标签进行比较 返回bool，在进行01转换求和出正确的总数\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    correct /=size\n",
    "    print(f\"{mode}Accuracy:{(100 * correct):>0.1f}%\\n\")\n",
    "    return correct\n",
    "\n",
    "total_loss = 0.\n",
    "best_acc = 0.\n",
    "for t in range(epoch):\n",
    "    print(f\"Epoch {t+1}/{epoch}\\n-------------------------------\")\n",
    "    total_loss = train_loop(train_dataloader, model, loss_fn, optimizer, lr_scheduler, t+1, total_loss)\n",
    "    valid_acc = test_loop(valid_dataloader, model, mode='Valid')\n",
    "    if valid_acc > best_acc:\n",
    "        best_acc = valid_acc\n",
    "        print('saving new weights...\\n')\n",
    "        torch.save(model.state_dict(), f'epoch_{t+1}_valid_acc_{(100*valid_acc):0.1f}_model_weights.bin')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a74c9",
   "metadata": {},
   "source": [
    "最后，我们加载验证集上最优的模型权重，汇报其在测试集上的性能。由于 AFQMC 公布的测试集上并没有标签，无法评估性能，这里我们暂且用验证集代替进行演示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee761c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhm18\\AppData\\Local\\Temp\\ipykernel_2348\\1258778870.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('./model/epoch_1_valid_acc_68.4_model_weights.bin'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestAccuracy:68.4%\n",
      "\n",
      "0.6835032437442076\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./model/epoch_1_valid_acc_68.4_model_weights.bin'))\n",
    "print(test_loop(valid_dataloader, model, mode='Test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8776a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
